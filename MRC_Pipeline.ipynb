{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRC_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x2Fpzwp7BF5Z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b0d23f02eb3149c59e0ee043e022d886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fcf5b5f6134243ff847be7e247ba9c4c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_887566e41e424df8a9546b885a5f39de",
              "IPY_MODEL_c5302aa8eed143b9a70fe86e31efb60e"
            ]
          }
        },
        "fcf5b5f6134243ff847be7e247ba9c4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "887566e41e424df8a9546b885a5f39de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_62073b25c301400695bc0fc9d914c023",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4602,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4602,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5bec0694701b486bb66410e3560a053c"
          }
        },
        "c5302aa8eed143b9a70fe86e31efb60e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_40bd66ba0118461f866560dbb8b1d8a3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.60k/4.60k [00:02&lt;00:00, 1.84kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_181ae90182cc49ec875154ee89cb1ad1"
          }
        },
        "62073b25c301400695bc0fc9d914c023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5bec0694701b486bb66410e3560a053c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40bd66ba0118461f866560dbb8b1d8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "181ae90182cc49ec875154ee89cb1ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d64d4082b48e4b9083007e631bd335d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3cf10ac7ad6e48778f86bb76f5e50900",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_469a4fa497e645c7a393d3b8bc567569",
              "IPY_MODEL_c5495136cc974a2691260482143fd0c1"
            ]
          }
        },
        "3cf10ac7ad6e48778f86bb76f5e50900": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "469a4fa497e645c7a393d3b8bc567569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5d42c9f18c2f4419bc5310724e6f5d62",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_828f70cc5e7b4efa97582e6a840cae9b"
          }
        },
        "c5495136cc974a2691260482143fd0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fa975f9b6f684d948621522b6250e539",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:01&lt;00:00, 123kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b8e5d9d07ab4436a853c93df90e68e7"
          }
        },
        "5d42c9f18c2f4419bc5310724e6f5d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "828f70cc5e7b4efa97582e6a840cae9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fa975f9b6f684d948621522b6250e539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b8e5d9d07ab4436a853c93df90e68e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6e3ee7f4b4d947f9b9fb298444b3f4e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dbcb4876d960430cafa22988088c2129",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b777c94bf0a94a85b87136a42894dbef",
              "IPY_MODEL_37d4aa4253014d638dea7a34ca9c67a7"
            ]
          }
        },
        "dbcb4876d960430cafa22988088c2129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b777c94bf0a94a85b87136a42894dbef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3b46b15c0e1c4bdd87453971411f4e44",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 112,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 112,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab1d6c4785f34c15bb7f9b9c4f3aadc5"
          }
        },
        "37d4aa4253014d638dea7a34ca9c67a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eeaf7d2752fc465a89a97b0ba373716b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 112/112 [00:00&lt;00:00, 183B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a2f5895a2da40f1b946a02c039b466b"
          }
        },
        "3b46b15c0e1c4bdd87453971411f4e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab1d6c4785f34c15bb7f9b9c4f3aadc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eeaf7d2752fc465a89a97b0ba373716b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a2f5895a2da40f1b946a02c039b466b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "afa61f857f704ceb8321564073b862aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5c8759bea55e41f89b1a9603e5ab26ee",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_60188a15d3a241158e05ffab3a55d62b",
              "IPY_MODEL_c65a4bcd5e544456971c88021e8a1b95"
            ]
          }
        },
        "5c8759bea55e41f89b1a9603e5ab26ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60188a15d3a241158e05ffab3a55d62b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d77a7787d0264766908955bdeccd4610",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 48,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 48,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e902f0ef8094d9ab6dc26c744851f08"
          }
        },
        "c65a4bcd5e544456971c88021e8a1b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2c711d48a3174f4d97998acd164776e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 48.0/48.0 [1:14:27&lt;00:00, 93.1s/B]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69569dc3e6f9489cae5f387a88653472"
          }
        },
        "d77a7787d0264766908955bdeccd4610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e902f0ef8094d9ab6dc26c744851f08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c711d48a3174f4d97998acd164776e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69569dc3e6f9489cae5f387a88653472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8cb96ae46a0b41df87acfd82bb14266b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6d0be926ebc44d69b68c2290ede82308",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f8335ad339d944f9b76f818833b1880e",
              "IPY_MODEL_82f645197ac94775a1158e2be80271cf"
            ]
          }
        },
        "6d0be926ebc44d69b68c2290ede82308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8335ad339d944f9b76f818833b1880e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_df67b9c7a35949da84944e30004f13d2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898822,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898822,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f44e5a7e55624cc4a94bc83e95aba635"
          }
        },
        "82f645197ac94775a1158e2be80271cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_915fec6b015e40d387a20a0533078beb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:02&lt;00:00, 429kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_667693de601f48e89f874c86de243ff5"
          }
        },
        "df67b9c7a35949da84944e30004f13d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f44e5a7e55624cc4a94bc83e95aba635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "915fec6b015e40d387a20a0533078beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "667693de601f48e89f874c86de243ff5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e19d482e8e994263be48d3539e86c84e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dd8fb1d643e6420f85cdaa15621e9700",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4914acf461664a6cac44edbd57a8ded4",
              "IPY_MODEL_cf7ded00be7d4ea19fee1f6e242ad4ab"
            ]
          }
        },
        "dd8fb1d643e6420f85cdaa15621e9700": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4914acf461664a6cac44edbd57a8ded4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c3aa00b5ba854190933a238c3af992d0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4711168e022d4246a64ea11ce14071df"
          }
        },
        "cf7ded00be7d4ea19fee1f6e242ad4ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_783311328787493cad2730db5eb8ca23",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:01&lt;00:00, 345kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5ad9958a02ec4b03afb8ebccd39ff205"
          }
        },
        "c3aa00b5ba854190933a238c3af992d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4711168e022d4246a64ea11ce14071df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "783311328787493cad2730db5eb8ca23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5ad9958a02ec4b03afb8ebccd39ff205": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "595c0888f3ad4dd49898a09fd8d28d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9f13c95751cd4190830bbf62aa67430b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_81feb01b4d14443d8655c7a741f1d8bc",
              "IPY_MODEL_a20c0a8e621246e0ba610496d53f3c95"
            ]
          }
        },
        "9f13c95751cd4190830bbf62aa67430b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81feb01b4d14443d8655c7a741f1d8bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_336b1ef46a12410ab5fd8370e409a4ad",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 772,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 772,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_772e7d0e0c9f4e38a0f5ca2c6bff97d9"
          }
        },
        "a20c0a8e621246e0ba610496d53f3c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b04bc1d81e8843d4ba625385e1860229",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 772/772 [00:00&lt;00:00, 1.06kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_49dfde9d51ca4e54a2972b1ed59b2c32"
          }
        },
        "336b1ef46a12410ab5fd8370e409a4ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "772e7d0e0c9f4e38a0f5ca2c6bff97d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b04bc1d81e8843d4ba625385e1860229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "49dfde9d51ca4e54a2972b1ed59b2c32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55859d3a36d4425b806152f097fb9c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_88245d6d451d40979c37d5ff333238c3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1d95d8db5b654355a978211c042eea20",
              "IPY_MODEL_547085a7591c40d2be45916bf377d025"
            ]
          }
        },
        "88245d6d451d40979c37d5ff333238c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1d95d8db5b654355a978211c042eea20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_91ef161cddc94f8f97c9606e2815dfb6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 26,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 26,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3708151d1a5a40608f7e32d292b5bbb4"
          }
        },
        "547085a7591c40d2be45916bf377d025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4dd0e61e52bc4417aa0f87e6b9d25f60",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 26.0/26.0 [00:00&lt;00:00, 262B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09681a23511d4b20864ee98e9f3da0fb"
          }
        },
        "91ef161cddc94f8f97c9606e2815dfb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3708151d1a5a40608f7e32d292b5bbb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4dd0e61e52bc4417aa0f87e6b9d25f60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09681a23511d4b20864ee98e9f3da0fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb490194def14fa4973d373f13033177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0b83a23ff1344e7dac4ff639c5bb858e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0ca8e3a40ceb47ee806c182fac2c9995",
              "IPY_MODEL_d90ad400a78c4a7aa9907b86381d6568"
            ]
          }
        },
        "0b83a23ff1344e7dac4ff639c5bb858e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ca8e3a40ceb47ee806c182fac2c9995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c5cedacc97064442ac5e7618414f69b4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2063600064,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2063600064,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a1f663618d04f09a6c9b6cd6106c6bd"
          }
        },
        "d90ad400a78c4a7aa9907b86381d6568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3fac9e5a585f412f939c28db77b27bfc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.06G/2.06G [00:41&lt;00:00, 50.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d334897e3aa04bba99612ae2e57c6ed8"
          }
        },
        "c5cedacc97064442ac5e7618414f69b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a1f663618d04f09a6c9b6cd6106c6bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3fac9e5a585f412f939c28db77b27bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d334897e3aa04bba99612ae2e57c6ed8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "tfile = \"Potârcă_JournMarFam_2015_P9O7.tei.xml\""
      ],
      "metadata": {
        "id": "59VPPQ4S_I8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = re.split(\"_|\\.\",tfile)\n",
        "print(s[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaYhQyP0_I7G",
        "outputId": "9a3b0bb0-2e78-4beb-b228-8ffb8e5d0c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P9O7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MGS1slT1_I3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z6vHC-C-_I0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heB3Va7nYrfk",
        "outputId": "adca7863-0863-47fd-dd31-5292501ea272"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDgUs1ESwMHn"
      },
      "source": [
        "# get the weights and model\n",
        "!cp -r /content/gdrive/MyDrive/Marsview_MRC/HLTC-MRQA/ /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oqys7JEWneq"
      },
      "source": [
        "# Step 1: THE RETRIVER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "X5G7p82u-BS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "206933ac-468a-45cb-b093-45c3ca64aa65"
      },
      "source": [
        "# Let's start by installing Haystack\n",
        "\n",
        "# Install the latest release of Haystack in your own environment\n",
        "#! pip install farm-haystack\n",
        "\n",
        "# Install the latest master of Haystack\n",
        "!pip install grpcio-tools==1.34.1\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git\n",
        "!wget --no-check-certificate https://dl.xpdfreader.com/xpdf-tools-linux-4.03.tar.gz\n",
        "!tar -xvf xpdf-tools-linux-4.03.tar.gz && sudo cp xpdf-tools-linux-4.03/bin64/pdftotext /usr/local/bin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting grpcio-tools==1.34.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/b8/086bc32788e7127f6655c1ec3bd000902e4c9225b587b19b09c4d8ced384/grpcio_tools-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from grpcio-tools==1.34.1) (57.0.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.5.0.post1 in /usr/local/lib/python3.7/dist-packages (from grpcio-tools==1.34.1) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.34.1 in /usr/local/lib/python3.7/dist-packages (from grpcio-tools==1.34.1) (1.34.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4.0dev,>=3.5.0.post1->grpcio-tools==1.34.1) (1.15.0)\n",
            "Installing collected packages: grpcio-tools\n",
            "Successfully installed grpcio-tools-1.34.1\n",
            "Collecting git+https://github.com/deepset-ai/haystack.git\n",
            "  Cloning https://github.com/deepset-ai/haystack.git to /tmp/pip-req-build-pp48r1pa\n",
            "  Running command git clone -q https://github.com/deepset-ai/haystack.git /tmp/pip-req-build-pp48r1pa\n",
            "Collecting farm==0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/c8/716e698e506b11df347529162890e29f4e1d71e90c09d1047748c8b6e77d/farm-0.8.0-py3-none-any.whl (204kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 7.0MB/s \n",
            "\u001b[?25hCollecting fastapi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/a8/a6be420593c4061c086e6d2ba47db46401d9af2b98b6cd33d35284f706d3/fastapi-0.65.2-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/fe/a41994c92897b162c0c83e8ef10bec54ebdefbce3f3725b530d2091492ac/uvicorn-0.14.0-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/dd/5b190393e6066286773a67dfcc2f9492058e9b57c4867a95f1ba5caf0a83/gunicorn-20.1.0-py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (1.1.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (0.0)\n",
            "Collecting elasticsearch<=7.10,>=7.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/ba/f950bdd9164fb2bbbe5093700162234fbe61f446fe2300a8993761c132ca/elasticsearch-7.10.0-py2.py3-none-any.whl (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 8.9MB/s \n",
            "\u001b[?25hCollecting elastic-apm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/d1/f9387c2419966905c8886e9d762ab68252299b7f3b42b9331e1a5b022eaa/elastic_apm-6.2.2-cp37-cp37m-manylinux2010_x86_64.whl (330kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 10.8MB/s \n",
            "\u001b[?25hCollecting tox\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/63/2fa635ac1b8a22e960654b07c270dfb53eb873aba261006536de40327b18/tox-3.23.1-py2.py3-none-any.whl (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: coverage in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (3.7.1)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 11.1MB/s \n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading https://files.pythonhosted.org/packages/46/40/a933ac570bf7aad12a298fc53458115cc74053474a72fbb8201d7dc06d3d/python-multipart-0.0.5.tar.gz\n",
            "Collecting python-docx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/a0/52729ce4aa026f31b74cc877be1d11e4ddeaa361dc7aebec148171644b33/python-docx-0.8.11.tar.gz (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 22.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (1.4.18)\n",
            "Collecting sqlalchemy_utils\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/1f/8bac49035ce1808b34607e1c00399f35329348c38cac93869f7fa30b0618/SQLAlchemy_Utils-0.37.7-py3-none-any.whl (100kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.3MB/s \n",
            "\u001b[?25hCollecting faiss-cpu>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/d6/072a9d18430b8c68c99ffb49fe14fbf89c62f71dcd4f5f692c7691447a14/faiss_cpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4MB 34.8MB/s \n",
            "\u001b[?25hCollecting tika\n",
            "  Downloading https://files.pythonhosted.org/packages/96/07/244fbb9c74c0de8a3745cc9f3f496077a29f6418c7cbd90d68fd799574cb/tika-1.24.tar.gz\n",
            "Collecting httptools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/87/ebb92639705924aac5ed6aa91bd4332b20d7180f3e94bd168742e7671025/httptools-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (3.2.5)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (8.8.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (2.5.1)\n",
            "Collecting pymilvus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/eb/1464c32113b6de44101dac58d1beb95c9ce4898139c8b69521135d431e19/pymilvus-1.1.2-py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n",
            "\u001b[?25hCollecting SPARQLWrapper\n",
            "  Downloading https://files.pythonhosted.org/packages/00/9b/443fbe06996c080ee9c1f01b04e2f683b2b07e149905f33a2397ee3b80a2/SPARQLWrapper-1.8.5-py3-none-any.whl\n",
            "Collecting mmh3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/46/0e568554c7f70ebc3de0c2b2effd3f7b25e66e0e4e0eacaeb7c9145949f2/mmh3-3.0.0-cp37-cp37m-manylinux2010_x86_64.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hCollecting weaviate-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/26/68aa4a2a797776785bc348e13dc91fed4fadbde2e88b09c9620567191838/weaviate_client-2.5.0-py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n",
            "\u001b[?25hCollecting psycopg2-binary\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/62/5f8eae172230141a53305150637fbdda7a535618d5dfa976dc013396837f/psycopg2_binary-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 31.9MB/s \n",
            "\u001b[?25hCollecting uvloop==0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/7a/54a80c03b555af21680a2f3692947b43a0d576d90c4c18cace0fee1ccc0e/uvloop-0.14.0-cp37-cp37m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 28.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (57.0.0)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/23/6b13608421ad2a6666b8ee64c301bcf9704123ef6b4579e6d362859e70c0/boto3-1.17.98-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (0.3.4)\n",
            "Collecting flask-restplus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/a6/b17c848771f96ad039ad9e3ea275e842a16c39c4f3eb9f60ee330b20b6c2/flask_restplus-0.13.0-py2.py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 27.2MB/s \n",
            "\u001b[?25hCollecting mlflow<=1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/4d/a6a4460e214842377dbc43d3e83bf976d564f7976822a9351adde60af44b/mlflow-1.13.1-py3-none-any.whl (14.1MB)\n",
            "\u001b[K     |████████████████████████████████| 14.2MB 222kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (1.4.1)\n",
            "Collecting dotmap\n",
            "  Downloading https://files.pythonhosted.org/packages/17/6f/c94adbb0e6d418ededbf1082a3067f178fb012573b960d446e5655e6fbe1/dotmap-1.3.23-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (2.23.0)\n",
            "Collecting Werkzeug==0.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/e4/a859d2fe516f466642fa5c6054fd9646271f9da26b0cac0d2f37fc858c8f/Werkzeug-0.16.1-py2.py3-none-any.whl (327kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 36.9MB/s \n",
            "\u001b[?25hCollecting torch<1.9,>1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/74/6fc9dee50f7c93d6b7d9644554bdc9692f3023fa5d1de779666e6bf8ae76/torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1MB 23kB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 86kB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (1.1.4)\n",
            "Collecting transformers==4.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 24.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (5.4.8)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (0.36.2)\n",
            "Collecting flask-cors\n",
            "  Downloading https://files.pythonhosted.org/packages/db/84/901e700de86604b1c4ef4b57110d4e947c218b9997adf5d38fa7da493bce/Flask_Cors-3.0.10-py2.py3-none-any.whl\n",
            "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f2/2d5425efe57f6c4e06cbe5e587c1fd16929dcf0eb90bd4d3d1e1c97d1151/pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 26.8MB/s \n",
            "\u001b[?25hCollecting starlette==0.14.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/34/db1890f442a1cd3a2c761f4109a0eb4e63503218d70a8c8e97faa09a5500/starlette-0.14.2-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.2MB/s \n",
            "\u001b[?25hCollecting asgiref>=3.3.4\n",
            "  Downloading https://files.pythonhosted.org/packages/17/8b/05e225d11154b8f5358e6a6d277679c9741ec0339d1e451c9cef687a9170/asgiref-3.3.4-py3-none-any.whl\n",
            "Collecting h11>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/0f/7a0eeea938eaf61074f29fed9717f2010e8d0e0905d36b38d3275a1e4622/h11-0.12.0-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from uvicorn->farm-haystack==0.9.0) (3.7.4.3)\n",
            "Requirement already satisfied: click>=7.* in /usr/local/lib/python3.7/dist-packages (from uvicorn->farm-haystack==0.9.0) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->farm-haystack==0.9.0) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->farm-haystack==0.9.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->farm-haystack==0.9.0) (2.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->farm-haystack==0.9.0) (0.22.2.post1)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch<=7.10,>=7.7->farm-haystack==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch<=7.10,>=7.7->farm-haystack==0.9.0) (2021.5.30)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tox->farm-haystack==0.9.0) (1.15.0)\n",
            "Requirement already satisfied: py>=1.4.17 in /usr/local/lib/python3.7/dist-packages (from tox->farm-haystack==0.9.0) (1.10.0)\n",
            "Requirement already satisfied: packaging>=14 in /usr/local/lib/python3.7/dist-packages (from tox->farm-haystack==0.9.0) (20.9)\n",
            "Requirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.7/dist-packages (from tox->farm-haystack==0.9.0) (0.10.2)\n",
            "Collecting pluggy>=0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from tox->farm-haystack==0.9.0) (4.5.0)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from tox->farm-haystack==0.9.0) (3.0.12)\n",
            "Collecting virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/08/f819421002e85a71d58368f7bffbe0b1921325e0e8ca7857cb5fb0e1f7c1/virtualenv-20.4.7-py2.py3-none-any.whl (7.2MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2MB 27.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx->farm-haystack==0.9.0) (4.2.6)\n",
            "Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.4.2->farm-haystack==0.9.0) (1.1.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->farm-haystack==0.9.0) (4.4.2)\n",
            "Requirement already satisfied: grpcio<1.38.0,>=1.22.0 in /usr/local/lib/python3.7/dist-packages (from pymilvus->farm-haystack==0.9.0) (1.34.1)\n",
            "Collecting ujson>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio-tools<1.38.0,>=1.22.0 in /usr/local/lib/python3.7/dist-packages (from pymilvus->farm-haystack==0.9.0) (1.34.1)\n",
            "Collecting rdflib>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 41.3MB/s \n",
            "\u001b[?25hCollecting validators>=0.18.2\n",
            "  Downloading https://files.pythonhosted.org/packages/db/2f/7fed3ee94ad665ad2c1de87f858f10a7785251ff75b4fd47987888d07ef1/validators-0.18.2-py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.98\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/1e/89106f14481e638c9397f533dde55b31d2ba4eeb6f3911f7d35fdf233127/botocore-1.20.98-py2.py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 30.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.0MB/s \n",
            "\u001b[?25hCollecting aniso8601>=0.82\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/04/e97c12dc034791d7b504860acfcdd2963fa21ae61eaca1c9d31245f812c3/aniso8601-9.0.1-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.8.0->farm-haystack==0.9.0) (2.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (3.13)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (0.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (1.3.0)\n",
            "Collecting prometheus-flask-exporter\n",
            "  Downloading https://files.pythonhosted.org/packages/f3/c1/2cc385fadf18dc75fe24c18899269eda4dcc60221d61eff7da4a6cc5c01d/prometheus_flask_exporter-0.18.2.tar.gz\n",
            "Collecting querystring-parser\n",
            "  Downloading https://files.pythonhosted.org/packages/88/6b/572b2590fd55114118bf08bde63c0a421dcc82d593700f3e2ad89908a8a9/querystring_parser-1.2.4-py2.py3-none-any.whl\n",
            "Collecting azure-storage-blob>=12.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/e5/ff9609a85f71cd41d759307b9d385ace34d29378e4750d0a0240aad535cb/azure_storage_blob-12.8.1-py2.py3-none-any.whl (345kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 24.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (3.12.4)\n",
            "Collecting gitpython>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/91/b38c4fabb6e5092ab23492ded4f318ab7299b19263272b703478038c0fbc/GitPython-3.1.18-py3-none-any.whl (170kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 40.4MB/s \n",
            "\u001b[?25hCollecting docker>=4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/5a/f988909dfed18c1ac42ad8d9e611e6c5657e270aa6eb68559985dbb69c13/docker-5.0.0-py2.py3-none-any.whl (146kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 42.5MB/s \n",
            "\u001b[?25hCollecting alembic<=1.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/e9/359dbb77c35c419df0aedeb1d53e71e7e3f438ff64a8fdb048c907404de3/alembic-1.4.1.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (0.4.1)\n",
            "Collecting databricks-cli>=0.8.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/af/631375abc29e59cedfa4467a5f7755503ba19898890751e1f2636ef02f92/databricks-cli-0.14.3.tar.gz (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.8.0->farm-haystack==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.8.0->farm-haystack==0.9.0) (2.10)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.8.0->farm-haystack==0.9.0) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.8.0->farm-haystack==0.9.0) (2.11.3)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 25.5MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1->farm==0.8.0->farm-haystack==0.9.0) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 26.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->farm-haystack==0.9.0) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14->tox->farm-haystack==0.9.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->tox->farm-haystack==0.9.0) (3.4.1)\n",
            "Requirement already satisfied: appdirs<2,>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0->tox->farm-haystack==0.9.0) (1.4.4)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/26/f6a23dd3e578132cf924e0dd5d4e055af0cd4ab43e2a9f10b7568bfb39d9/distlib-0.3.2-py2.py3-none-any.whl (338kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 36.0MB/s \n",
            "\u001b[?25hCollecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: prometheus_client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (0.11.0)\n",
            "Collecting azure-core<2.0.0,>=1.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/2f/3d25c8dfea7c9bdf96142f019bfb47bd08dca3ce0eea6511fdeecf281e87/azure_core-1.15.0-py2.py3-none-any.whl (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 42.0MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 27.8MB/s \n",
            "\u001b[?25hCollecting msrest>=0.6.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/cc/6c96bfb3d3cf4c3bdedfa6b46503223f4c2a4fa388377697e0f8082a4fed/msrest-0.6.21-py2.py3-none-any.whl (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.9MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/5f/3c211d168b2e9f9342cfb53bcfc26aab0eac63b998015e7af7bcae66119d/websocket_client-1.1.0-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[?25hCollecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/54/dbc07fbb20865d3b78fdb7cf7fa713e2cba4f87f71100074ef2dc9f9d1f7/Mako-1.1.4-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.9MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (0.8.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->farm==0.8.0->farm-haystack==0.9.0) (2.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (1.14.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.18->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (1.3.0)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (2.20)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (3.1.1)\n",
            "Building wheels for collected packages: farm-haystack, langdetect, python-multipart, python-docx, tika, seqeval, prometheus-flask-exporter, alembic, databricks-cli\n",
            "  Building wheel for farm-haystack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for farm-haystack: filename=farm_haystack-0.9.0-cp37-none-any.whl size=180827 sha256=c941e9e235a579a3593941fc9e72e33020d140ab553b5259aa37cce002338b00\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rfyuu9w7/wheels/ab/41/a4/4fbf362de283352078ecb6705c08b6525347aaea2eead2a60c\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-cp37-none-any.whl size=993242 sha256=5f9a74d939bd1a80e5631ba03c376d8d2f1f499afe2620c9f4f910eb556c8092\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/18/13/038c34057808931c7ddc6c92d3aa015cf1a498df5a70268996\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-cp37-none-any.whl size=31677 sha256=c2216384ddeb8a847a6fe9a1143ea188b7c2980c4b2489127aae050e1a64f085\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/e6/66/14a866a3cbd6a0cabfbef91f7edf40aa03595ef6c88d6d1be4\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-cp37-none-any.whl size=184508 sha256=63c30366a5adb992f9eae957368968d2c1a76ec0a805aec8303d23596ac43aff\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/90/f1/a7cb70b38633ae04e7fb963b1c70f63fd6fc01c075b8230adc\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-cp37-none-any.whl size=32894 sha256=650bd476a05a8a9e1a41535e8e301553cddf873a52d4e49d3779c745551f8b57\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/9c/f5/0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16184 sha256=8f1865c510ef565b0f202affecd7237164c2da19c7cb7155509b22c1bd890c26\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.2-cp37-none-any.whl size=17415 sha256=5fdc457b7fb2d7689889a684d6d88ec727387390e799f9c82bff8cfe10c2fd49\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/e2/9c/4f3ee23964802940f81a8b476d0b9be6fb6348cb12df2e2226\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158170 sha256=b08ed5342942a5f25af8f2a735fd429525d88e74659e7acd90cbe39cb8758a0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/07/f7/12f7370ca47a66030c2edeedcc23dec26ea0ac22dcb4c4a0f3\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.14.3-cp37-none-any.whl size=100560 sha256=11d95b61184a48ac1534d8aef33d9747bd7ddad0c05fed532a736a226d3af153\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/24/f3/34d8e3964dac4ba849d844273c49a679111b00d5799ebb934a\n",
            "Successfully built farm-haystack langdetect python-multipart python-docx tika seqeval prometheus-flask-exporter alembic databricks-cli\n",
            "\u001b[31mERROR: torchvision 0.10.0+cu102 has requirement torch==1.9.0, but you'll have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pytest 3.6.4 has requirement pluggy<0.8,>=0.5, but you'll have pluggy 0.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.20.98 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: weaviate-client 2.5.0 has requirement tqdm>=4.59.0, but you'll have tqdm 4.41.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, aniso8601, flask-restplus, prometheus-flask-exporter, querystring-parser, azure-core, cryptography, isodate, msrest, azure-storage-blob, gunicorn, smmap, gitdb, gitpython, websocket-client, docker, Mako, python-editor, alembic, databricks-cli, mlflow, dotmap, Werkzeug, torch, sentencepiece, seqeval, sacremoses, huggingface-hub, tokenizers, transformers, flask-cors, farm, pydantic, starlette, fastapi, asgiref, h11, uvicorn, elasticsearch, elastic-apm, pluggy, distlib, virtualenv, tox, langdetect, python-multipart, python-docx, sqlalchemy-utils, faiss-cpu, tika, httptools, ujson, pymilvus, rdflib, SPARQLWrapper, mmh3, validators, weaviate-client, psycopg2-binary, uvloop, farm-haystack\n",
            "  Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "Successfully installed Mako-1.1.4 SPARQLWrapper-1.8.5 Werkzeug-0.16.1 alembic-1.4.1 aniso8601-9.0.1 asgiref-3.3.4 azure-core-1.15.0 azure-storage-blob-12.8.1 boto3-1.17.98 botocore-1.20.98 cryptography-3.4.7 databricks-cli-0.14.3 distlib-0.3.2 docker-5.0.0 dotmap-1.3.23 elastic-apm-6.2.2 elasticsearch-7.10.0 faiss-cpu-1.7.1.post2 farm-0.8.0 farm-haystack-0.9.0 fastapi-0.65.2 flask-cors-3.0.10 flask-restplus-0.13.0 gitdb-4.0.7 gitpython-3.1.18 gunicorn-20.1.0 h11-0.12.0 httptools-0.2.0 huggingface-hub-0.0.8 isodate-0.6.0 jmespath-0.10.0 langdetect-1.0.9 mlflow-1.13.1 mmh3-3.0.0 msrest-0.6.21 pluggy-0.13.1 prometheus-flask-exporter-0.18.2 psycopg2-binary-2.9.1 pydantic-1.8.2 pymilvus-1.1.2 python-docx-0.8.11 python-editor-1.0.4 python-multipart-0.0.5 querystring-parser-1.2.4 rdflib-5.0.0 s3transfer-0.4.2 sacremoses-0.0.45 sentencepiece-0.1.96 seqeval-1.2.2 smmap-4.0.0 sqlalchemy-utils-0.37.7 starlette-0.14.2 tika-1.24 tokenizers-0.10.3 torch-1.8.1 tox-3.23.1 transformers-4.6.1 ujson-4.0.2 uvicorn-0.14.0 uvloop-0.14.0 validators-0.18.2 virtualenv-20.4.7 weaviate-client-2.5.0 websocket-client-1.1.0\n",
            "--2021-06-22 08:18:59--  https://dl.xpdfreader.com/xpdf-tools-linux-4.03.tar.gz\n",
            "Resolving dl.xpdfreader.com (dl.xpdfreader.com)... 45.79.72.155\n",
            "Connecting to dl.xpdfreader.com (dl.xpdfreader.com)|45.79.72.155|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23024720 (22M) [application/x-gzip]\n",
            "Saving to: ‘xpdf-tools-linux-4.03.tar.gz’\n",
            "\n",
            "xpdf-tools-linux-4. 100%[===================>]  21.96M  36.0MB/s    in 0.6s    \n",
            "\n",
            "2021-06-22 08:19:00 (36.0 MB/s) - ‘xpdf-tools-linux-4.03.tar.gz’ saved [23024720/23024720]\n",
            "\n",
            "xpdf-tools-linux-4.03/\n",
            "xpdf-tools-linux-4.03/ANNOUNCE\n",
            "xpdf-tools-linux-4.03/bin32/\n",
            "xpdf-tools-linux-4.03/bin32/pdftotext\n",
            "xpdf-tools-linux-4.03/bin32/pdfinfo\n",
            "xpdf-tools-linux-4.03/bin32/pdftopng\n",
            "xpdf-tools-linux-4.03/bin32/pdfimages\n",
            "xpdf-tools-linux-4.03/bin32/pdftoppm\n",
            "xpdf-tools-linux-4.03/bin32/pdftops\n",
            "xpdf-tools-linux-4.03/bin32/pdfdetach\n",
            "xpdf-tools-linux-4.03/bin32/pdffonts\n",
            "xpdf-tools-linux-4.03/bin32/pdftohtml\n",
            "xpdf-tools-linux-4.03/CHANGES\n",
            "xpdf-tools-linux-4.03/bin64/\n",
            "xpdf-tools-linux-4.03/bin64/pdftotext\n",
            "xpdf-tools-linux-4.03/bin64/pdfinfo\n",
            "xpdf-tools-linux-4.03/bin64/pdftopng\n",
            "xpdf-tools-linux-4.03/bin64/pdfimages\n",
            "xpdf-tools-linux-4.03/bin64/pdftoppm\n",
            "xpdf-tools-linux-4.03/bin64/pdftops\n",
            "xpdf-tools-linux-4.03/bin64/pdfdetach\n",
            "xpdf-tools-linux-4.03/bin64/pdffonts\n",
            "xpdf-tools-linux-4.03/bin64/pdftohtml\n",
            "xpdf-tools-linux-4.03/INSTALL\n",
            "xpdf-tools-linux-4.03/COPYING3\n",
            "xpdf-tools-linux-4.03/doc/\n",
            "xpdf-tools-linux-4.03/doc/pdftotext.1\n",
            "xpdf-tools-linux-4.03/doc/pdfdetach.1\n",
            "xpdf-tools-linux-4.03/doc/pdftopng.1\n",
            "xpdf-tools-linux-4.03/doc/sample-xpdfrc\n",
            "xpdf-tools-linux-4.03/doc/xpdfrc.5\n",
            "xpdf-tools-linux-4.03/doc/pdftops.1\n",
            "xpdf-tools-linux-4.03/doc/pdffonts.1\n",
            "xpdf-tools-linux-4.03/doc/pdftoppm.1\n",
            "xpdf-tools-linux-4.03/doc/pdfimages.1\n",
            "xpdf-tools-linux-4.03/doc/pdftohtml.1\n",
            "xpdf-tools-linux-4.03/doc/pdfinfo.1\n",
            "xpdf-tools-linux-4.03/COPYING\n",
            "xpdf-tools-linux-4.03/README\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l1cMPKLY2ve",
        "outputId": "c00a83f2-723b-4a63-b463-fefe123aa35a"
      },
      "source": [
        "pip install jsonlines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/58/06f430ff7607a2929f80f07bfd820acbc508a4e977542fefcc522cde9dff/jsonlines-2.0.0-py3-none-any.whl\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KZAy92QE-BTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a9a997-fdb9-43b4-b506-b058f047a020"
      },
      "source": [
        "# Here are the imports we need\n",
        "\n",
        "from haystack.file_converter.txt import TextConverter\n",
        "from haystack.file_converter.pdf import PDFToTextConverter\n",
        "from haystack.file_converter.docx import DocxToTextConverter\n",
        "\n",
        "from haystack.preprocessor.preprocessor import PreProcessor\n",
        "from haystack import Finder\n",
        "from haystack.preprocessor.cleaning import clean_wiki_text\n",
        "from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\n",
        "from haystack.reader.farm import FARMReader\n",
        "from haystack.reader.transformers import TransformersReader\n",
        "from haystack.utils import print_answers\n",
        "\n",
        "from typing import List\n",
        "import requests\n",
        "import pandas as pd\n",
        "from haystack import Document\n",
        "from haystack.document_store.faiss import FAISSDocumentStore\n",
        "from haystack.generator.transformers import RAGenerator\n",
        "from haystack.retriever.dense import DensePassageRetriever\n",
        "\n",
        "\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from haystack import Document\n",
        "\n",
        "import regex as re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/22/2021 08:19:11 - INFO - faiss.loader -   Loading faiss with AVX2 support.\n",
            "06/22/2021 08:19:11 - INFO - faiss.loader -   Could not load library with AVX2 support due to:\n",
            "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
            "06/22/2021 08:19:11 - INFO - faiss.loader -   Loading faiss.\n",
            "06/22/2021 08:19:11 - INFO - faiss.loader -   Successfully loaded faiss.\n",
            "06/22/2021 08:19:11 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fYgCVjpYqNC"
      },
      "source": [
        "# Recommended: Start Elasticsearch using Docker\n",
        "! docker run -d -p 9200:9200 -e \"discovery.type=single-node\" elasticsearch:7.9.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qnvVTPHGTkf"
      },
      "source": [
        "# In Colab / No Docker environments: Start Elasticsearch from source\n",
        "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "! tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "! chown -R daemon:daemon elasticsearch-7.9.2\n",
        "\n",
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'],\n",
        "                   stdout=PIPE, stderr=STDOUT,\n",
        "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                  )\n",
        "# wait until ES has started\n",
        "! sleep 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bA8uyXs8GTkg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f3e6712-ea70-4c41-a735-537ea43bd64a"
      },
      "source": [
        "# Connect to Elasticsearch\n",
        "\n",
        "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
        "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/22/2021 08:19:54 - INFO - elasticsearch -   HEAD http://localhost:9200/ [status:200 request:0.106s]\n",
            "06/22/2021 08:19:55 - INFO - elasticsearch -   PUT http://localhost:9200/document [status:200 request:0.543s]\n",
            "06/22/2021 08:19:55 - INFO - elasticsearch -   PUT http://localhost:9200/label [status:200 request:0.254s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHhif7gDy2P0"
      },
      "source": [
        "!mkdir /content/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IBvb8gedQtf",
        "outputId": "5a37ad64-7aad-442a-b862-625e64353e48"
      },
      "source": [
        "## Final method for splitting.\n",
        "\n",
        "all_docs = convert_files_to_dicts(dir_path=\"/content/data\")\n",
        "preprocessor = PreProcessor(\n",
        "    clean_empty_lines=True,\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=False,\n",
        "    split_by=\"word\",\n",
        "    split_length=150,\n",
        "    split_respect_sentence_boundary=True\n",
        ")\n",
        "nested_docs = [preprocessor.process(d) for d in all_docs]\n",
        "docs = [d for x in nested_docs for d in x]\n",
        "\n",
        "print(f\"n_files_input: {len(all_docs)}\\nn_docs_output: {len(docs)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/22/2021 08:32:21 - INFO - haystack.preprocessor.utils -   Converting /content/data/m4.txt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "n_files_input: 1\n",
            "n_docs_output: 47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH2ZsNOMKy-v",
        "outputId": "43d576f7-c8e4-4d5b-cffe-d9085b302ef7"
      },
      "source": [
        "document_store.delete_all_documents()\n",
        "document_store.write_documents(docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/22/2021 08:32:25 - WARNING - haystack.document_store.elasticsearch -   DEPRECATION WARNINGS: \n",
            "                1. delete_all_documents() method is deprecated, please use delete_documents method\n",
            "                For more details, please refer to the issue: https://github.com/deepset-ai/haystack/issues/1045\n",
            "                \n",
            "06/22/2021 08:32:26 - INFO - elasticsearch -   POST http://localhost:9200/document/_delete_by_query [status:200 request:0.129s]\n",
            "06/22/2021 08:32:29 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.217s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMiXCWvASTnS"
      },
      "source": [
        "from haystack.retriever.sparse import ElasticsearchRetriever\n",
        "retriever = ElasticsearchRetriever(document_store=document_store)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34IsptrH9eol"
      },
      "source": [
        "QUESTIONS = [\n",
        " \"Who was the winner of the 50$ gift card?\",\n",
        " \"What reason Esther Yoon gave for partnering with otter Ai?\",\n",
        " \"Who was moderating the Q&A panel?\",\n",
        " \"Who is Esther Yoon?\",\n",
        " \"Where one can find the Zoom Topia videos?\",\n",
        " \"What is Zoom Topia video?\",\n",
        " \"What was the value of gift card?\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bx9qTth96rE",
        "outputId": "cd57ff7d-8de5-44dc-9a17-786a248baa2b"
      },
      "source": [
        "in_reader = []\n",
        "\n",
        "for question in QUESTIONS:\n",
        "  a = retriever.retrieve(query=question, top_k = 3)\n",
        "\n",
        "  flat_docs_dict: Dict[str, Any] = {}\n",
        "  for document in a:\n",
        "      for k, v in document.__dict__.items():\n",
        "        # print(v)\n",
        "          if k not in flat_docs_dict:\n",
        "              flat_docs_dict[k] = []\n",
        "          flat_docs_dict[k].append(v)\n",
        "\n",
        "  in_context = ' '.join(flat_docs_dict['text'])\n",
        "\n",
        "  # Disct for Xlnet input\n",
        "  x_in =  {'context': 'context input', 'qas': [{'question': \"question\", 'answers': [], 'qid': 'update', 'question_tokens': [], 'is_impossible': False, 'detected_answers': [{'text': '', 'token_spans': [], 'char_spans': []}]}]}\n",
        "  #get the detials from the customer\n",
        "  x_in['context'] = in_context\n",
        "  for s in x_in['qas']:\n",
        "    s['qid'] = 'customer_1'\n",
        "    s['question'] = question\n",
        "  \n",
        "  in_reader.append(x_in)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/22/2021 08:32:31 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.082s]\n",
            "06/22/2021 08:32:31 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.014s]\n",
            "06/22/2021 08:32:31 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.010s]\n",
            "06/22/2021 08:32:31 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.011s]\n",
            "06/22/2021 08:32:31 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.015s]\n",
            "06/22/2021 08:32:31 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.011s]\n",
            "06/22/2021 08:32:31 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.010s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Feo4L2C0tu"
      },
      "source": [
        "## Extarct New question on same doc by Ak"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv1i3CpzC0ll"
      },
      "source": [
        "QUESTIONS = [\n",
        " \"Do you want to say something about the Bruce Bulger campaign? And what's going on there?\",\n",
        " \"How much is too much noise?\",\n",
        " \"Sg&a is a percent of total expense and hcv a what does that mean?\",\n",
        " \"What do we know about net debt market cap?\",\n",
        " \"So Kim, do you think that many companies have zero revenues or we just miss you towing?\",\n",
        " \"How can you have negative Revenue?\",\n",
        " \"How do I get out of here?\",\n",
        " \"Can zoom be integrated with Google meet?\",\n",
        " \"Can zoom know if Raj is speaking?\",\n",
        " \"Will Product Manager takes care of sales?\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOaf6DBIRLox"
      },
      "source": [
        "QUESTIONS = [\n",
        " \"Who was the winner of the 50$ gift card?\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5lkTOaMRScI",
        "outputId": "99168ab7-192b-4115-cdc2-e5e3e4bd607a"
      },
      "source": [
        "for question in QUESTIONS:\n",
        "  a = retriever.retrieve(query=question, top_k = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 15:53:21 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.022s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDWtwkDhRUNK",
        "outputId": "4d4fd983-77de-49fb-ad52-3a48c04c1880"
      },
      "source": [
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': \"It is all very simple and very quick and efficient. And yeah. Yeah it all just works with The key to zoom, right and I think you guys continue surprising us with new features and functions and that versatility of it just works has been maintained. So it was really great to see it was great. When you to join us after Bobby. - (33:33)  SPEAKER: Bobby Swartz\\nYou probably have somebody that's going to get a surprise surprise. Yes. Thank you. Everyone for jumping in. This was really good to get to know a little bit more on the backside of Zoom for those who are able to attend. The show so thank you for coming out. We do have a winner of a $50 gift card today. It is Lashawn Jackson. Thank you very much. Mark.\", 'score': 15.737869, 'probability': 0.8773136667338791, 'question': None, 'meta': {'_split_id': 45, 'name': 'm4.txt'}, 'embedding': None, 'id': '1547dbcb9d7bbac22e907a8f7a68bccb'},\n",
              " {'text': \"They are called meat and they announced the neat bar and the neat board which is a touch display and then Paul Lee announced. Their Appliance x-series. So there is an x 30 which is for smaller rooms. And then there's an x 54 larger rooms and those specs can actually be found on their website. If you guys want to know what's the difference in terms of x 30 and x 50 in my okay on time Bobby. You're fine. You're right Esther. Hey Greg, so think about you know in layman's terms. What is this room's Appliance? It's enterprise-grade video conferencing without Enterprise size complications. These are for people who don't want to do. Thinking they don't want to add multiple cameras. They're just like give me a box and I just wanted to plug it in real quick.\", 'score': 4.737468, 'probability': 0.6438659850046042, 'question': None, 'meta': {'_split_id': 21, 'name': 'm4.txt'}, 'embedding': None, 'id': 'ed6ca46e200a6304a355e66d34c6ee2f'},\n",
              " {'text': \"So keep those coming in and we will we will get to as many as we can one of the other things that came out as in Zoom topia that I wanted to talk about briefly was the new PowerPoint virtual background. Oh, yeah. That was a new annamma moment. - (22:49)  SPEAKER: Esther Yoon\\nYeah, it really takes you beyond the traditional video conferencing use case where it feels like a face-to-face just conversation to an interactive literally like Vanna White approach to communication. Right? So it's almost like you turn on the TV and it feels like it should be a one-way stream, but you can now communicate back to someone who sharing content literally interacting physically with the content. That's behind them.\", 'score': 3.4550433, 'probability': 0.6063226040644973, 'question': None, 'meta': {'_split_id': 30, 'name': 'm4.txt'}, 'embedding': None, 'id': 'a1bf3b5a8a4f66608abd018e2f705d57'},\n",
              " {'text': \"Vendors which I'll go over and this is really for scaling to smaller and midsize conference rooms. So think about more standardized room deployments. The three big things is it has to be easy to procure globally. It has to be easy to scale globally and it has to be easy to manage at skill. So think just larger deployments. This is really where the value of appliances come come in. So like I mentioned you get everything that you love with zoom rooms, you get the DIY aspect for those who are cost conscious you have the customization aspect for those who are extra creative and want more of a out-of-this-world zoom room setup. You have the off-the-shelf use case where you can pick and choose the vendors of your choice, right?\", 'score': 3.325202, 'probability': 0.6024419258081309, 'question': None, 'meta': {'_split_id': 18, 'name': 'm4.txt'}, 'embedding': None, 'id': '104fa5aa728cf557f56c809f93c9dca2'},\n",
              " {'text': \"Yeah, and so here's kind of a look at the products x 30 x 50. The neat bar is going to be so Polly is going to be available for pre-order end of November. Neat bar is going to be is available for pre-order now. So the one on the left this is going to be available for pre-order on me got no in January and then D 10 this is available for pre-order now and I forgot to mention that they actually announced a 27-inch Appliance of specifically for the executive office and focus room use cases and this is actually really cool because it's priced at $599. Just my yeah, it's yeah, I couldn't believe it when they told me that they could price it at that that low. So yeah, basically in a nutshell that was the zoom Topia Zoom rooms announcements in a nutshell.\", 'score': 3.2487218, 'probability': 0.6001500186712349, 'question': None, 'meta': {'_split_id': 26, 'name': 'm4.txt'}, 'embedding': None, 'id': 'e716ae2874a3f84953e4f5f3d4312410'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsVBgFI0C0jU",
        "outputId": "3a748cf3-dffe-4f86-a6ef-41e3f81c1726"
      },
      "source": [
        "in_reader = []\n",
        "\n",
        "for question in QUESTIONS:\n",
        "  a = retriever.retrieve(query=question, top_k = 3)\n",
        "\n",
        "  flat_docs_dict: Dict[str, Any] = {}\n",
        "  for document in a:\n",
        "      for k, v in document.__dict__.items():\n",
        "        # print(v)\n",
        "          if k not in flat_docs_dict:\n",
        "              flat_docs_dict[k] = []\n",
        "          flat_docs_dict[k].append(v)\n",
        "\n",
        "  in_context = ' '.join(flat_docs_dict['text'])\n",
        "\n",
        "  # Disct for Xlnet input\n",
        "  x_in =  {'context': 'context input', 'qas': [{'question': \"question\", 'answers': [], 'qid': 'update', 'question_tokens': [], 'is_impossible': False, 'detected_answers': [{'text': '', 'token_spans': [], 'char_spans': []}]}]}\n",
        "  #get the detials from the customer\n",
        "  x_in['context'] = in_context\n",
        "  for s in x_in['qas']:\n",
        "    s['qid'] = 'customer_1'\n",
        "    s['question'] = question\n",
        "  \n",
        "  in_reader.append(x_in)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/22/2021 08:34:26 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.020s]\n",
            "06/22/2021 08:34:26 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.010s]\n",
            "06/22/2021 08:34:26 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.011s]\n",
            "06/22/2021 08:34:26 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.011s]\n",
            "06/22/2021 08:34:26 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.011s]\n",
            "06/22/2021 08:34:26 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.009s]\n",
            "06/22/2021 08:34:26 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.009s]\n",
            "06/22/2021 08:34:27 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.009s]\n",
            "06/22/2021 08:34:27 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.008s]\n",
            "06/22/2021 08:34:27 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.016s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egVSb_leC0ey"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Fpzwp7BF5Z"
      },
      "source": [
        "### No use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYbZviS6STk_",
        "outputId": "02dbd949-c88a-4c1e-fa46-1b7100a2b9a9"
      },
      "source": [
        "a = retriever.retrieve(query=\"From where Richard Branson was supposed to call?\", top_k = 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 12:09:48 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.013s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4gt-lJ9JSoG"
      },
      "source": [
        "# Parsing the retriever output into a python dict object\n",
        "flat_docs_dict: Dict[str, Any] = {}\n",
        "for document in a:\n",
        "    for k, v in document.__dict__.items():\n",
        "      # print(v)\n",
        "        if k not in flat_docs_dict:\n",
        "            flat_docs_dict[k] = []\n",
        "        flat_docs_dict[k].append(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJOc_8JCJSUV"
      },
      "source": [
        "# Joining the top_k outputs\n",
        "in_context = ' '.join(flat_docs_dict['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "tIworwn2N949",
        "outputId": "b24ada9e-cc73-431a-a036-0a81b9cacd18"
      },
      "source": [
        "in_context"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Yep that and we Richard Branson he was supposed to be calling in from Necker Island, which would have been I would have loved that to show like no fan with like even compromise bandwidth and we can have a crystal-clear call. But yeah, you know, we truly wanted to show the power of video at Zoom Topia. - (23:53)  SPEAKER: Melissa Dillman\\nYeah, it was very impressive. Okay. So those were my two Rockstar kind of moments, but let me get to what what all of our guests want to know. Peoples are maybe they are but people still use them all the time and there are scenarios, especially in the modern Workforce where you're like Hey, we're talking about a project. I want to share this with you, but I can't because I'm on a phone call. Let me describe you the content on my screen. It's super weird. And so what we are looking at is okay. So collaboration and communication should be seamless, even when the needs for collaboration communication and change in real time. So I'm sending out this. We saw the phone, right? We saw some come this year. We implemented it. And yeah, obviously, I'm pretty I'm pretty deep with zoom. I use it everyday. I was a little concerned. Like, how do I have a call and how do I get it to a meeting and I have to say it's amazingly intuitive just like everything else was in right? - (20:56)  SPEAKER: Esther Yoon\\nYeah, it's so easy.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAAXNcnA4kuw"
      },
      "source": [
        "# Disct for Xlnet input\n",
        "x_in =  {'context': 'Want love.', 'qas': [{'question': \"What did Luther do at the end of his speech?\", 'answers': [], 'qid': '8452474afb0a40f1904a5571b001b6d1', 'question_tokens': [], 'is_impossible': False, 'detected_answers': [{'text': '', 'token_spans': [], 'char_spans': []}]}]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6mMHd_d2juL"
      },
      "source": [
        "#get the detials from the customer\n",
        "x_in['context'] = in_context\n",
        "for s in x_in['qas']:\n",
        "  s['qid'] = 'customer_1'\n",
        "  s['question'] = \"From where Richard Branson was supposed to call?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYmbSyLLAOkh",
        "outputId": "6b7e9bc5-9ea5-4b2c-f310-11613b8b3981"
      },
      "source": [
        "x_in"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': \"Yep that and we Richard Branson he was supposed to be calling in from Necker Island, which would have been I would have loved that to show like no fan with like even compromise bandwidth and we can have a crystal-clear call. But yeah, you know, we truly wanted to show the power of video at Zoom Topia. - (23:53)  SPEAKER: Melissa Dillman\\nYeah, it was very impressive. Okay. So those were my two Rockstar kind of moments, but let me get to what what all of our guests want to know. Peoples are maybe they are but people still use them all the time and there are scenarios, especially in the modern Workforce where you're like Hey, we're talking about a project. I want to share this with you, but I can't because I'm on a phone call. Let me describe you the content on my screen. It's super weird. And so what we are looking at is okay. So collaboration and communication should be seamless, even when the needs for collaboration communication and change in real time. So I'm sending out this. We saw the phone, right? We saw some come this year. We implemented it. And yeah, obviously, I'm pretty I'm pretty deep with zoom. I use it everyday. I was a little concerned. Like, how do I have a call and how do I get it to a meeting and I have to say it's amazingly intuitive just like everything else was in right? - (20:56)  SPEAKER: Esther Yoon\\nYeah, it's so easy. - (21:49)  SPEAKER: Melissa Dillman\\nAll send it out Esther. It's okay. We have a follow-up. Okay, we will put it no problem. It really is amazing how fast it works? Yeah. Oh convenient. It has become to be on a phone call and be like yesterday. I did it and the guy I was talking to your he's like, well, how do we get into the meeting and I'm like, we're already there. It was so seamless. He didn't even realize we had already moved. What we really wanted to do was just have a little pet about coming up the hill so that show on what is in the future for the next coming up on Zoom. What were some of the big announcements and then most importantly from all of you in attendance? What are your questions that we have from Zoom?\",\n",
              " 'qas': [{'answers': [],\n",
              "   'detected_answers': [{'char_spans': [], 'text': '', 'token_spans': []}],\n",
              "   'is_impossible': False,\n",
              "   'qid': 'customer_1',\n",
              "   'question': 'From where Richard Branson was supposed to call?',\n",
              "   'question_tokens': []}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAh-YuolZJCJ"
      },
      "source": [
        "# store the input file\n",
        "# !mkdir /content/data\n",
        "!mkdir /content/data/dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWPJuo5T344v"
      },
      "source": [
        "# last worked Formate the input like this\n",
        "\n",
        "import jsonlines\n",
        "with jsonlines.open('/content/data/dev/DROP-piece.jsonl', mode='w') as writer:\n",
        "    writer.write({'header': {'dataset': 'RACE', 'mrqa_split': 'dev'}})\n",
        "    writer.write(x_in)\n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fDBd_5SBJWr"
      },
      "source": [
        "### till here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cdgrmlHJjCV"
      },
      "source": [
        "{\"context\": \"- (33:33)  SPEAKER: Bobby Swartz\\nYou probably have somebody that's going to get a surprise surprise. Yes. Thank you. Everyone for jumping in. This was really good to get to know a little bit more on the backside of Zoom for those who are able to attend. The show so thank you for coming out. We do have a winner of a $50 gift card today. It is Lashawn Jackson. Thank you very much. Mark. Decker will be reaching out to you to go ahead and provide that everyone attendees. So think just larger deployments. This is really where the value of appliances come come in. So like I mentioned you get everything that you love with zoom rooms, you get the DIY aspect for those who are cost conscious you have the customization aspect for those who are extra creative and want more of a out-of-this-world zoom room setup. You have the off-the-shelf use case where you can pick and choose the vendors of your choice, right? We have one same thing dedicated mobile cart because when all of our rooms get busy suddenly somebody is dragging the card round and we do use on Wi-Fi because I know Melissa and I have personally wheeled into our warehouse to have some impromptu meetings, but things are getting a little busy in the office and it's a great use case. - (32:43)  SPEAKER: Melissa Dillman\\nIt works flawlessly. It really does. We also have one of our one of our friends has put together a challenge our sales Executives. And that is to video record and broadcast.\", \"qas\": [{\"question\": \"Who was the winner of the 50$ gift card?\", \"answers\": [], \"qid\": \"customer_11\", \"question_tokens\": [], \"is_impossible\": false, \"detected_answers\": [{\"text\": \"\", \"token_spans\": [], \"char_spans\": []}]}]}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1iRyhewfhUY"
      },
      "source": [
        "!mkdir /content/data/dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml360YC_ASqk"
      },
      "source": [
        "import jsonlines\n",
        "with jsonlines.open('/content/data/dev/DROP-piece_bad.jsonl', mode='w') as writer:\n",
        "    writer.write({'header': {'dataset': 'MARSVIEW', 'mrqa_split': 'dev'}})\n",
        "    for item in in_reader:\n",
        "      writer.write(item)\n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALQXT9z6mGei"
      },
      "source": [
        "### Test for top k answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vo9CsGNPqyy",
        "outputId": "15cf129e-771b-4720-d6eb-4cbb9c780128"
      },
      "source": [
        "pip install jsonlines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/58/06f430ff7607a2929f80f07bfd820acbc508a4e977542fefcc522cde9dff/jsonlines-2.0.0-py3-none-any.whl\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZyfvhIJkf6h"
      },
      "source": [
        "with open('/content/m4.txt', 'r') as file:\n",
        "    data = file.read()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "muCPqRSJk9Fh",
        "outputId": "df27b372-4a68-4c25-cd8c-e4e4f5f8a836"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"- (00:05)  SPEAKER: Bobby Swartz\\nHello. Hello, thank you. All of our attendees as we left this room populate here. I'll just keep on monologuing a little bit. Welcome everybody to the November Edition, which is actually here in October of these staring Town Hall. So I just got back from a whirlwind trip to his little tiny show called zootopia and it was I was just commenting before the show starts. I still feel a little tired because it was a whirlwind of fun talking. And customers talking to our partners talking to new partners talking to our vendor Partners talking to our friends and zoom just filled with tons of great conversation a lot of excitement a lot of new features coming to the zoom room capabilities and zoom platform is a whole and I wanted to thank we've got Esther Yoon who is the product marketing manager for conference room Solutions joining us today coming off the heels of zootopia as well, which is kind of a big deal in the zoom World shuts down, San Jose. Pretty good. What we really wanted to do was just have a little pet about coming up the hill so that show on what is in the future for the next coming up on Zoom. What were some of the big announcements and then most importantly from all of you in attendance? What are your questions that we have from Zoom? So I've heard that's there for a long time as has Melissa here as we're always joined and I guess a she is a bit of a rock star when it comes to being able to get things done being able to answer questions. And then let's put you on the spot but I've always been generally pleased with our conversations on you know, whenever we eat something or we need a real answer about what what the future of Zoom entails or how to do something or really just what the right solution is. You're always there for us and you're always there for the partners that work with you. So appreciate that typical format down at the bottom of your screams there. You see the Q&A page go ahead and click that. You can start asking your questions now anything that comes to mind. The Q&A panel will be moderated by Melissa Dillman and estar let you take it away for a few minutes. Tell us a little bit about yourself and a little bit if you want to give a synopsis some of those six things from the show get people's minds thinking on what the future of zoom and Tails what they might have missed if they weren't there which I think a lot of people might not have been and will done not take it straight to the questions. \\n\\n- (02:28)  SPEAKER: Esther Yoon\\nThank you everybody. Yeah, of course, you know think you guys first and foremost for having me here. My name is Esther Yoon. I'm the product marketer for our conference room Solutions. And so at a high level that is zoom rooms our Flagship conference room offering that's conference room connector connector, which is our interop solution, but it also includes the value add services such as digital signage and scheduling display, which hopefully is very beneficial to you guys as well. I want to start off by saying, you know from a from zooms perspective. Our success is really going to be tied to our customer success and if sarin is successful making, you know their customers happy then in the end it all trickles back up to zoom success as well. So that's why I'm here and that's why we work so closely together because our success is inherently tied together, right? And so with that, you know, it's been a pleasure to work with, you know, the Dillman folks the building family and also Bobby and the rest of the team we've done a lot of really cool programs such as strategic. Eating programs you guys have built a lot of really bespoke capabilities for us on your guys's platform so that we can actually deliver happiness to our customers and that's why so many people at Zoom here love working with the staring the Starin family because you guys take things, you know, you guys take a very modern approach, right? You guys do things differently and that is very in line with how Zoom likes to do things. We don't like to do things the way things have always Has been done just for the sake of doing it, right? We like to start from scratch and say hey, what's the best way to do it and we've found so much success working closely with you know are staring family and doing so all the way from PSO2 seating programs. So yeah, I know that was a long-winded feel but thank you guys so much. Yeah, so, you know, one of the things that I'm really really excited actually do you mind if I have I have some slides maybe it'll help their \\n\\n- (04:34)  SPEAKER: Bobby Swartz\\nNot much text absolutely. \\n\\n- (04:36)  SPEAKER: Esther Yoon\\nBring them all and awesome. Okay. So I'm not a words kind of person but I do I'm a very visual individual and so yeah, I wanted to share with you guys kind of what we announced at zootopia regarding Zoom rooms. One thing that was really really exciting for me because I've heard this be a pain point for many of our customers was one the pain Point stem from hey, I have a panel for room control systems and then I have observed rooms controller panel and they both have different UI. One they reserve the the room control panel for typically their board rooms because it just means like an extra cost. Now. We have zoom rooms native Native room control integration. And that's the ability to take any IP things command and integrate it into your Zoomers controller. So typically what you would see for the Enterprise would be things like light control your projector your AC your blinds, but the cool thing about this is because it can you know, really control anything that takes an IP Command when I demoed this at our sales training actually had an LED Wireless belt on owes a debt that we controlled from the zoo rooms controller. I had like a little to be you know, what the car washes you have those like little dancing to these gold that so really showing the versatility and yeah, you can support traditional room control use cases, but you can also Troll non-traditional use cases. It's kind of up to the customer and we don't have much documentation on this but you know, I know Sam Cooke Keiko who's on our PSO team is working on documentation. This capability is going to be rolled out by the end of this year. So I'm apologize if you guys are asking questions on how do we do this? We'll be able to answer that really really shortly. Yeah, stay tuned. You guys will get documentation soon as it's available. Now this one is let's see if it plays so we have a find a room map integration capability. So essentially the use case for this is you know, you walk up to your display and it's let's let's pause here. It's busy and you're like, okay you look around and all the rooms around you are busy. Well with find a room app integration, you can actually click this button in the top right corner that says reserved other rooms were actually working. On making it more discoverable because right now it's a little subtle. You can click that and see a list of all the rooms nearby that are available or on your floor that are available and then you can reserve it straight from there. Now, if you don't want to upload a map you can actually see a list view of all the available rooms and they're going to be in alphabetical order. So that capability is it was it's g a so if you guys have scheduling display, I would recommend testing it out you upload the map as a jpeg. Or PNG directly through the zoom admin portal. It's super easy. I guarantee you training is going to take less than three minutes. And basically you reserved the room. You can name the meaning add participants. So it's added to their calendar and there's a I'm just going to Breeze through it and then a visitor it preserves the room and then you're golden. Okay, so that's that. This is kind of what it looks like on the back end. It's slightly changed now, so I apologize for having this out. Edit image but it's really as simple as you upload the jpeg or PNG file and you can just draw where the rooms are and then assign your Zoom rooms that are already, you know in your admin portal to these different spaces and you can do this at the floor level. The other thing that we announced is companion whiteboard now this should be really exciting for you guys because this is essentially another way for you guys to bring Hardware into an existing Zoom rooms Zoom room environment. So without having without the customer having to purchase an additional software license if they wanted to bring a touch display in a compute as a dedicated interactive whiteboard. So let's say they have an existing room that with the standard displays that they're like, hey we To replace our whiteboard with an interactive display. They can actually leverage their existing room license just bring in a touch display with a computer. It actually doesn't need peripherals. But if you have like let's say a D7 you can use that as well. Then you can automatically sync it to that room and so great opportunity for you guys to just kind of bring more Hardware into an existing Zoom room space. So yeah, there's it right there. And then this one this is this was something I was very passionate about because one of the biggest differentiators about Zoom rooms is the ability to scale to different use cases. I know you I'm talking to the group of experts here, but you guys know about the multicam functionality you guys know how we can scale up to like training rooms very easily. Well, we've now added this capability where if you bring a green screen you're able to have a virtual background for your Zoom room. So this is actually one of the Setups that we had at Zoom Topia like front and center and we actually conducted live webinars and streamed it straight to Facebook live and here's the setup and I have the equipment list I could send over if you guys want to know you guys will probably make your own actually with our PSO team. But this was just a really cool demonstration of you know, Zoom rooms is great for huddle rooms conference rooms and basically any physical any other type of physical environment or use case where you want to leverage, you know physical environment, right? It and connect it to video. So here's kind of what it looked like back in and everything is controlled by the zoom room controller. And this was like our mock-up so our mock-up and our actual deployment turned out to be quite similar and then okay. So the other the last thing that I want to bring to you guys is, you know, we announced a big new program. It's it's really it's stem from what customers were looking for right? You guys know like flexibility and options are huge when it comes to customers. They want Hardware. Commendations reference designs, but they also want to be like, hey, I work only with this vendor. I want to bring them in totally get that number two. They want simplified procurement. Now you guys have done a really great job making procurement simple, you know on a global level level that becomes a challenge right when you have customers that are like, hey, I have an I have my HQ in the US, but I need a ship to 60 different countries and I want to minimize the number of vendors involved and then lastly. Lee management one of the largest frustrations that we have what customers that are deploying videos at scale is you know, let's say they have a hundred rooms and 90 of those are like small huddle rooms or conference rooms. They want to be able to manage those quickly and do the end-to-end software stack even down to the device firmware. Now, you know currently we had some friction points in these different types of Milestones. So Hardware procuring Hardware installation management user experience. Now what we're trying to do was make that entire customer Journey more streamline and frictionless specifically for the Huddle rooms, the global large-scale global huddle room in conference room deployments. So with that said, you know, I want to preface it with zoom booms will always maintain that open Hardware ecosystem for our customers. That is kind of a core part of our Our DNA and it allows for you know, powerful customization, you know support for bespoke use cases, you know, you can really optimize your AV especially for those large spaces. And and so that is that's not going away. This is an addition to our Hardware options. We wanted to say, hey, we have all of this. Oh, but then there's more now so we've announced a purpose-built appliance program with select Hardware. Vendors which I'll go over and this is really for scaling to smaller and midsize conference rooms. So think about more standardized room deployments. The three big things is it has to be easy to procure globally. It has to be easy to scale globally and it has to be easy to manage at skill. So think just larger deployments. This is really where the value of appliances come come in. So like I mentioned you get everything that you love with zoom rooms, you get the DIY aspect for those who are cost conscious you have the customization aspect for those who are extra creative and want more of a out-of-this-world zoom room setup. You have the off-the-shelf use case where you can pick and choose the vendors of your choice, right? We have the single vendor kits for people were like, I don't want to think I just want like a Windows iot set up, you know a Logitech has a lot of these great Solutions. You could still have multicam and still leverage all the Abilities of Zoom rooms and then we have remote software updates for our Zoom rooms application. But now with our Zoom rooms appliances you can have remote for more updates for all the peripherals on the appliances. You can have a lockdown OS designed to only run Zoom rooms. And then you can have that Global streamlined Hardware procurement option, which this is kind of like an asterisk because this is someone available. It's just kind of it seems to be a little bit more difficult to find for some customers. So the three partners that we announced with our Pauly D 10 and neat and actually, let me see if I have a better image. Sorry. Okay. So this is the d-10 D7 55 inch all-in-one. You guys are very familiar with the windows iot version. They are rolling out with a appliance version that's completely locked down now. I I talked to a lot of customers about this. They want the options for both. They're like awesome windows iot. Is great because it lets me still add my my additional peripherals that I want. Like if they want a multicam set up they can have that if someone is just like I want it to be completely locked out. I don't want anything. I want it at face value like the D7 appliances is a great solution. We have a company a partner that announced their company. They are called meat and they announced the neat bar and the neat board which is a touch display and then Paul Lee announced. Their Appliance x-series. So there is an x 30 which is for smaller rooms. And then there's an x 54 larger rooms and those specs can actually be found on their website. If you guys want to know what's the difference in terms of x 30 and x 50 in my okay on time Bobby. You're fine. You're right Esther. Hey Greg, so think about you know in layman's terms. What is this room's Appliance? It's enterprise-grade video conferencing without Enterprise size complications. These are for people who don't want to do. Thinking they don't want to add multiple cameras. They're just like give me a box and I just wanted to plug it in real quick. And I want to do a hundred of these at skill and have it work. That's that's really what it comes down to these are appliances meaning they were built from the ground up all the way down to the OS so their Tech they're technically run on Android operating systems, and they're totally custom to the zoom rooms experience and the way Way to purchase these Solutions are going to be different. We'll meet specifically neat will only be selling through their website. So they're going to take more of an e-commerce only approach Polly will be available through their select Channel network and the to everybody on the call the poly and the D7 products. \\n\\n- (16:48)  SPEAKER: Bobby Swartz\\nThose are all available through the channel through stare in here in the US. \\n\\n- (16:52)  SPEAKER: Esther Yoon\\nYep. Yep, and that's really great because if you think about it between these two partners you have your Sighs or sorry small conference rooms with the x30 midsize conference rooms with the x 50 and if you want to zoom rooms for touch you have the d-10 so it hits about 90% of use cases from what I've seen. Installation is extremely easy for the customer, right you guys have awesome getting started guides and you're getting started guys are going to get even more compelling because you're going to be able to show like hey for the controller. You just get the peoe connect it awesome. Then you have your your bar your ex 30 or x50 connect it to display connected to power connected to ethernet for data. It's kind of like a consumer experience just like you would set up a display with a sound system at home. And they had a video like Polly had a video where they were showing the insulation. It literally took two minutes. And then enhanced management, I already covered this but you know through the zoom admin portal click update and it does the entire software stack even down to the device firmware. So in a nutshell the way we're starting to structure. Our Zoom rooms is is really, you know, first and foremost it comes down to what does the customer want if they want that flexibility in their Huddle's basis and conference rooms. We're going to recommend them a Windows iot computer Mac or something like that so they can have that bespoke experience if they want that. Hey, I don't want to think I want that TurnKey solution. We're going to recommend a certain rooms Appliance for large rooms and bespoke use cases we say Eat your heart out. You can have multi cam you can have you know, the peripherals peripherals of your of your choice. Here's just the basic components that you need to have a zoom room. And you know, they can work with AV integrators. I know you guys have a really strong A/V integrator Network to build these bespoke room use cases. And so in a nutshell our announcements are taking away from anything. We're not saying. Hey, we're end of life being this we're doing a Next Generation version of that. It's not at all. It's all additive. It's everything that you had before plus more love it. Yeah, and so here's kind of a look at the products x 30 x 50. The neat bar is going to be so Polly is going to be available for pre-order end of November. Neat bar is going to be is available for pre-order now. So the one on the left this is going to be available for pre-order on me got no in January and then D 10 this is available for pre-order now and I forgot to mention that they actually announced a 27-inch Appliance of specifically for the executive office and focus room use cases and this is actually really cool because it's priced at $599. Just my yeah, it's yeah, I couldn't believe it when they told me that they could price it at that that low. So yeah, basically in a nutshell that was the zoom Topia Zoom rooms announcements in a nutshell. \\n\\n- (20:03)  SPEAKER: Melissa Dillman\\nOkay, Esther, I mean that was great. I mean you got an awesome you covered a lot but you know what you left out. \\n\\n- (20:13)  SPEAKER: Esther Yoon\\nWhat the phone ah, okay. \\n\\n- (20:17)  SPEAKER: Melissa Dillman\\nJust watched your video. \\n\\n- (20:19)  SPEAKER: Esther Yoon\\nAgain, and how cool you were with the phone? \\n\\n- (20:22)  SPEAKER: Melissa Dillman\\nThat's hilarious and I have to say we just so zootopia last year. We saw the phone, right? We saw some come this year. We implemented it. And yeah, obviously, I'm pretty I'm pretty deep with zoom. I use it everyday. I was a little concerned. Like, how do I have a call and how do I get it to a meeting and I have to say it's amazingly intuitive just like everything else was in right? \\n\\n- (20:56)  SPEAKER: Esther Yoon\\nYeah, it's so easy. Yeah, I could send out the link in this group if that helps and that way we don't have to watch it now, but if you guys want to watch it on your guys's free time. We just uploaded it to YouTube but in a nutshell, what what Zoom is trying to do is drive a video first experience, right? But we know that phone systems aren't going away anytime soon. Peoples are maybe they are but people still use them all the time and there are scenarios, especially in the modern Workforce where you're like Hey, we're talking about a project. I want to share this with you, but I can't because I'm on a phone call. Let me describe you the content on my screen. It's super weird. And so what we are looking at is okay. So collaboration and communication should be seamless, even when the needs for collaboration communication and change in real time. So I'm sending out this. \\n\\n- (21:49)  SPEAKER: Melissa Dillman\\nAll send it out Esther. It's okay. We have a follow-up. Okay, we will put it no problem. It really is amazing how fast it works? Yeah. Oh convenient. It has become to be on a phone call and be like yesterday. I did it and the guy I was talking to your he's like, well, how do we get into the meeting and I'm like, we're already there. It was so seamless. He didn't even realize we had already moved. Huge fan love the zoom phone that's working for me loved your video you and your videos are amazing. So I enjoyed that people are writing as questions. So keep those coming in and we will we will get to as many as we can one of the other things that came out as in Zoom topia that I wanted to talk about briefly was the new PowerPoint virtual background. Oh, yeah. That was a new annamma moment. \\n\\n- (22:49)  SPEAKER: Esther Yoon\\nYeah, it really takes you beyond the traditional video conferencing use case where it feels like a face-to-face just conversation to an interactive literally like Vanna White approach to communication. Right? So it's almost like you turn on the TV and it feels like it should be a one-way stream, but you can now communicate back to someone who sharing content literally interacting physically with the content. That's behind them. \\n\\n- (23:14)  SPEAKER: Melissa Dillman\\nIt's yeah, I was absolutely impressed by our product management team and you know The other thing that I don't know if it travelled across the networks as much but the fact that you all brought an orchestra into the Key net in John Williams has Zoom meeting. \\n\\n- (23:33)  SPEAKER: Esther Yoon\\nYeah, that was incredible. Yep that and we Richard Branson he was supposed to be calling in from Necker Island, which would have been I would have loved that to show like no fan with like even compromise bandwidth and we can have a crystal-clear call. But yeah, you know, we truly wanted to show the power of video at Zoom Topia. \\n\\n- (23:53)  SPEAKER: Melissa Dillman\\nYeah, it was very impressive. Okay. So those were my two Rockstar kind of moments, but let me get to what what all of our guests want to know. So I see a couple of questions like well, yeah, what all is new with zoom and what we can offer to our customers. I think we've covered a lot of that. We've got a lot of news. Obviously coming out one of the other things I know this isn't on the question yet, but it should be that was touched on briefly. I think is really important was the carbon footprint hmm in talking about that reduction within the carbon footprint because we can do everything via video. Yep. You don't have to travel the miles. I hate to fly. So anytime I can do it via video. I'm all for it. But that was really interesting. As well presume rooms and Native control integration. How much of the GUI? Can we customize? It's a great question. \\n\\n- (24:56)  SPEAKER: Esther Yoon\\nYeah, that is um, so you can actually customize quite a bit. So for example, let me see if I can find you could do kind of those carrot notches so you can do incremental like if you do an AC you could set temperatures so you could do up down you can make a binary on or off and you can actually customize the text being shown you could have different modes. So for example, At Zoom Topia we had turn on the lights but it wasn't on or off it was do you want shooting star effect? You want meteor shower. Do you want the black lasers lasers? So you can have different modalities of a different of a control system that are preset. And yeah, we're looking at expanding all those capabilities, but those are the three main ones which is the binary direct kind of Notch up and Notch down, so bring it up bring it down and then also the modes and I'm guessing \\n\\n- (25:48)  SPEAKER: Melissa Dillman\\nFor the folks that we speak with the most right now. We can't put branding on it like a you know, if it's a corporate background image, but I imagine that probably comes soon. \\n\\n- (26:02)  SPEAKER: Esther Yoon\\nYeah, I wouldn't get out for sure because it's something that people have asked for that, you know branding is a huge part of large Enterprises and that's something our teams definitely exploring. Awesome. \\n\\n- (26:13)  SPEAKER: Melissa Dillman\\nNext question we have is is scheduling a display ad scheduling display available be \\n\\n- (26:19)  SPEAKER: Esther Yoon\\nAndroid yes it is. It's available on Android for portrait and kind of wide standard view. \\n\\n- (26:26)  SPEAKER: Melissa Dillman\\nAwesome in our someone's asking about this Zoom Topia videos. It's a big topic within the industry of dreaming the events and talking about having that access because not everybody can fly out San Jose and get out there. \\n\\n- (26:43)  SPEAKER: Esther Yoon\\nCan you tell everybody where they can find those videos? Yeah, if you go on zootopia .us In about a week and a half will have all those uploaded on there as recording. So all the breakout sessions as well as all the product key notes and Main stage sessions and I know I watch even though I was at zootopia last year. \\n\\n- (27:03)  SPEAKER: Melissa Dillman\\nThere was so much going on you couldn't get to everyone. So I watched a bunch of them when I came home as you know to get a chance to watch it again or enjoy it again. So it's awesome some rooms native control integration. How much can the going to be customized? Okay, we talked about that you are going to be using IP Control. So that will be being bailed out more and more. Correct. \\n\\n- (27:28)  SPEAKER: Esther Yoon\\nEverything is IP based as long as they can I pee command. Yep. All right kids Optical device. Yep. \\n\\n- (27:36)  SPEAKER: Melissa Dillman\\nSo that'll be great. One of the other things that came out was the simultaneous interpretation. Yep. So, okay. It left us all scratching our heads. How does that work? \\n\\n- (27:48)  SPEAKER: Esther Yoon\\nYou have a bunch of interpreters that are living inside my zoom or what we'll actually uh, so what it so a lot of times large organizations will have translators for things like town halls. And so what we're doing is offering this capability where you can have interpreter channels and what happens is let's say Bobby you were talking to everybody and you have hopefully one day teams in Japan Australia China like, you know everywhere globally and maybe english isn't their first language, you can have a you know, someone who's bilingual and those local regions jump on As an interpreter and they can listen to you and translate what you're saying into a interpreter Channel and let's say if I was like Hey, I want to hear the Italian translation of what Bobby saying I could find it if there's an interpreter signed and what happens is Bobby's voice would go down to 20% and I would hear my interpreter voice at 80% and we do I will do that is because cuz tone is such a big part of communication. We want to understand when if Bobby's yelling or super happy and super excited like you want to be able to hear those nuances, and so will actually be able to support unlimited interpreters if you if you had like, let's say a hundred we can actually support that. \\n\\n- (29:07)  SPEAKER: Melissa Dillman\\nCan you imagine Bobby being yelling? \\n\\n- (29:09)  SPEAKER: Esther Yoon\\nI can't actually work that I was like, that's a silly thought one day. \\n\\n- (29:14)  SPEAKER: Bobby Swartz\\nMaybe we'll see. \\n\\n- (29:18)  SPEAKER: Melissa Dillman\\nThat was funny. The other thing that came out with talking about the transcription that it can capture action items. That was really interesting. Can you tell everybody about that? \\n\\n- (29:31)  SPEAKER: Esther Yoon\\nYeah, so we are partnering with otter Ai and we partnered with them because they have a really high level of accuracy when it comes to translations. They do a really good job at picking up things like accents and little internal nuances a lot better than we've seen a couple of other vendors and basically when you open up the door for live transcription, there's a lot of things that you can there's a lot of things that it opens up in terms of artificial intelligence. So live transcription is great. If let's say you just want to passively view you can view just what's being said, but from there think about what happens when you have this pull of text, you can start extracting keywords like okay Melissa, I'll take that as an action item we can find those trigger words highlight them and kind of give you a cliff notes version of let's say you had a two-hour meeting. You don't want to have to watch a two-hour meeting recording right? You want to be present in those in those let's say brainstorming sessions and then you want your meeting tools to be able to intelligently sift out all the stuff that actually is important to you, right? So that's really what we're trying to do is make collaboration and working easy. Her we don't want you to have to be taking notes, you know, we want you to be physically present with your remote and person participants in the other people were there it's intervene they were amazing. \\n\\n- (31:02)  SPEAKER: Melissa Dillman\\nBut now I know that in all my meetings, I will be using the term Bobby action item frequently just so that it picks it up and just Bobby so, you know, okay, so I'm running over a little bit here. Esther I know you're extremely busy. I was so grateful that you were able to join us. But let's hit one last question. This is a great question when you're dealing with the mobile car version of the Zoomer what happens when we unplug and plug in move it around etcetera. Yeah experience has been you don't have to reload everything it holds everything for you. \\n\\n- (31:41)  SPEAKER: Esther Yoon\\nI get out also send you a video of what that experience looks like because I have a demonstration. It's a forty seven second video of me. Me pushing around a D7 cart and I'm already signed in or this room. Let's say like I think it was signed in as like D7 car product team or whatever because the product team they like to just drag this wherever they are. And so essentially once that room is assigned to a zoom room. You can just plug it in and you can actually use it over Wi-Fi. We don't recommend Wi-Fi, but you can technically and once you plug in and Boot It Up in about 47 seconds you are in a zoom room. \\n\\n- (32:19)  SPEAKER: Melissa Dillman\\nIt just takes a couple seconds to load up. Yeah, and it's not reloading Keys. It's not it all just comes right back. \\n\\n- (32:25)  SPEAKER: Bobby Swartz\\nWe do this in our office here quite a bit. We have one same thing dedicated mobile cart because when all of our rooms get busy suddenly somebody is dragging the card round and we do use on Wi-Fi because I know Melissa and I have personally wheeled into our warehouse to have some impromptu meetings, but things are getting a little busy in the office and it's a great use case. \\n\\n- (32:43)  SPEAKER: Melissa Dillman\\nIt works flawlessly. It really does. We also have one of our one of our friends has put together a challenge our sales Executives. And that is to video record and broadcast. They're putting together a zoom rooms D7 with a cart it proving that even a vice-president of sales can do it in under 20 minutes. It is all very simple and very quick and efficient. And yeah. Yeah it all just works with The key to zoom, right and I think you guys continue surprising us with new features and functions and that versatility of it just works has been maintained. So it was really great to see it was great. When you to join us after Bobby. \\n\\n- (33:33)  SPEAKER: Bobby Swartz\\nYou probably have somebody that's going to get a surprise surprise. Yes. Thank you. Everyone for jumping in. This was really good to get to know a little bit more on the backside of Zoom for those who are able to attend. The show so thank you for coming out. We do have a winner of a $50 gift card today. It is Lashawn Jackson. Thank you very much. Mark. Decker will be reaching out to you to go ahead and provide that everyone attendees. I know we had a few other questions come in here. What we'll be doing is we'll respond to each of you personally and everybody some of the details and links that were shared videos and other collateral we will get that out to you in the follow-up email that comes as well as the recording of this you can share with coworkers. Refer back to anything a lot of great information here as always as there. Thank you very much for taking the time out to join us all and run through these details on it's always pleasure. \\n\\n- (34:28)  SPEAKER: Esther Yoon\\nAwesome. My pleasure. Bye everybody. Bye everyone. \\n\\n- (34:31)  SPEAKER: Unassigned\\nThank you. \\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5o3ig0lPUDD"
      },
      "source": [
        "# last worked Formate the input like this\n",
        "\n",
        "import jsonlines\n",
        "with jsonlines.open('/content/data/dev/DROP-piece.jsonl', mode='w') as writer:\n",
        "    writer.write({'header': {'dataset': 'MARSVIEW', 'mrqa_split': 'dev'}})\n",
        "    writer.write({\"context\": data, \"qas\": [{\"question\": \"Sg&a is a percent of total expense and hcv a what does that mean?\", \"answers\": [], \"qid\": \"customer_13\", \"question_tokens\": [], \"is_impossible\": False, \"detected_answers\": [{\"text\": \"\", \"token_spans\": [], \"char_spans\": []}]}]})\n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyxp0-6Q6YRY"
      },
      "source": [
        "# Step 2 THE READER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z5IFoUJnGEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a6fed8-43ef-4ad5-d8d0-0c863b895c99"
      },
      "source": [
        "cd /content/HLTC-MRQA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/HLTC-MRQA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJhOEOtsnGEx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7445d28d-60e0-4c3a-bdc8-c257c5a5fcc5"
      },
      "source": [
        "pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/42/7bddc0d5bc169596fbd13b6e1b844f832491ab671381e483da3bf5292ca9/tensorflow_gpu-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (410.9MB)\n",
            "\u001b[K     |████████████████████████████████| 411.0MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonlines in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.1.96)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.34.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 31.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.19.5)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 31.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (57.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (0.16.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=875d53d669cb87b2d8489ed3be8ad5b338a743da23a859b030bdf76d1c277cb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement gast==0.4.0, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorflow-estimator<2.6.0,>=2.5.0rc0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, keras-applications, tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvCTfWliDM0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193c86ba-7080-4186-85d4-1fb000717c11"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huA86or0iNJF",
        "outputId": "6859db01-eafe-4910-c64e-76b179a7b8d3"
      },
      "source": [
        "cd /content/HLTC-MRQA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/HLTC-MRQA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntwtlJCqWYlO"
      },
      "source": [
        "# coding=utf-8\n",
        "\n",
        "\n",
        "####### To get top 5 predicton change the code in the ===== n_best_size\n",
        "########     To change the output answer length ============  max_answer_length\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl import flags\n",
        "import absl.logging as _logging  # pylint: disable=unused-import\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import six\n",
        "import random\n",
        "import gc\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "if six.PY2:\n",
        "  import cPickle as pickle\n",
        "else:\n",
        "  import pickle\n",
        "import jsonlines\n",
        "\n",
        "import tensorflow as tf\n",
        "import sentencepiece as spm\n",
        "from src.prepro_utils import preprocess_text, encode_ids, encode_pieces, printable_text\n",
        "import src.function_builder as function_builder\n",
        "import src.model_utils as model_utils\n",
        "import src.mrqa_utils\n",
        "from src.data_utils import SEP_ID, CLS_ID, VOCAB_SIZE\n",
        "\n",
        "# Psudo flag then delete it and then create all req flags\n",
        "# tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "\n",
        "# def del_all_flags(FLAGS):\n",
        "flags_dict = FLAGS._flags()\n",
        "keys_list = [keys for keys in flags_dict]\n",
        "for keys in keys_list:\n",
        "  FLAGS.__delattr__(keys)\n",
        "\n",
        "# FLAGS = tf.app.flags.FLAGS\n",
        "\n",
        "SPIECE_UNDERLINE = u'▁'\n",
        "\n",
        "SEG_ID_P   = 0\n",
        "SEG_ID_Q   = 1\n",
        "SEG_ID_CLS = 2\n",
        "SEG_ID_PAD = 3\n",
        "\n",
        "# Preprocessing\n",
        "flags.DEFINE_bool(\"do_prepro\", default=False,\n",
        "      help=\"Perform preprocessing only.\")\n",
        "flags.DEFINE_integer(\"num_proc\", default=16,\n",
        "      help=\"Number of preprocessing processes.\")\n",
        "flags.DEFINE_integer(\"proc_id\", default=0,\n",
        "      help=\"Process id for preprocessing.\")\n",
        "\n",
        "# Model\n",
        "flags.DEFINE_string(\"model_config_path\", default=\"/content/gdrive/MyDrive/HLTC-MRQA/model-weights/xlnet_config.json\",\n",
        "      help=\"Model config path.\")\n",
        "flags.DEFINE_float(\"dropout\", default=0.1,\n",
        "      help=\"Dropout rate.\")\n",
        "flags.DEFINE_float(\"dropatt\", default=0.1,\n",
        "      help=\"Attention dropout rate.\")\n",
        "flags.DEFINE_integer(\"clamp_len\", default=-1,\n",
        "      help=\"Clamp length.\")\n",
        "flags.DEFINE_string(\"summary_type\", default=\"last\",\n",
        "      help=\"Method used to summarize a sequence into a vector.\")\n",
        "flags.DEFINE_bool(\"use_bfloat16\", default=False,\n",
        "      help=\"Whether to use bfloat16.\")\n",
        "\n",
        "# Parameter initialization\n",
        "flags.DEFINE_enum(\"init\", default=\"normal\",\n",
        "                  enum_values=[\"normal\", \"uniform\"],\n",
        "                  help=\"Initialization method.\")\n",
        "flags.DEFINE_float(\"init_std\", default=0.02,\n",
        "                   help=\"Initialization std when init is normal.\")\n",
        "flags.DEFINE_float(\"init_range\", default=0.1,\n",
        "                   help=\"Initialization std when init is uniform.\")\n",
        "\n",
        "# I/O paths\n",
        "flags.DEFINE_bool(\"overwrite_data\", default=False,\n",
        "                  help=\"If False, will use cached data if available.\")\n",
        "flags.DEFINE_string(\"init_checkpoint\", default=\"/content/gdrive/MyDrive/HLTC-MRQA/model-weights/model.ckpt-15000\",\n",
        "                    help=\"checkpoint path for initializing the model. \"\n",
        "                    \"Could be a pretrained model or a finetuned model.\")\n",
        "flags.DEFINE_bool(\"init_global_vars\", default=False,\n",
        "                  help=\"If true, init all global vars. If false, init \"\n",
        "                  \"trainable vars only.\")\n",
        "flags.DEFINE_string(\"output_dir\", default=\"\",\n",
        "                    help=\"Output dir for TF records.\")\n",
        "flags.DEFINE_string(\"predict_dir\", default=\"\",\n",
        "                    help=\"Dir for predictions.\")\n",
        "flags.DEFINE_string(\"spiece_model_file\", default=\"/content/gdrive/MyDrive/HLTC-MRQA/spiece.model\",\n",
        "                    help=\"Sentence Piece model path.\")\n",
        "flags.DEFINE_string(\"model_dir\", default=\"experiment/mrqa\",\n",
        "                    help=\"Directory for saving the finetuned model.\")\n",
        "flags.DEFINE_string(\"train_dir\", default=\"\",\n",
        "                    help=\"Path of train directory.\")\n",
        "flags.DEFINE_string(\"dev_dir\", default=\"/content/data/dev\",\n",
        "                    help=\"Path of development directory.\")\n",
        "\n",
        "# Data preprocessing config\n",
        "flags.DEFINE_integer(\"max_seq_length\",\n",
        "                     default=512, help=\"Max sequence length\")\n",
        "flags.DEFINE_integer(\"max_query_length\",\n",
        "                     default=128, help=\"Max query length\")\n",
        "flags.DEFINE_integer(\"doc_stride\",\n",
        "                     default=128, help=\"Doc stride\")\n",
        "flags.DEFINE_integer(\"max_answer_length\",\n",
        "                     default=128, help=\"Max answer length\")                      ########################### MAX ANSWER LENGTH ###########################\n",
        "flags.DEFINE_bool(\"uncased\", default=False, help=\"Use uncased data.\")\n",
        "\n",
        "# TPUs and machines\n",
        "flags.DEFINE_bool(\"use_tpu\", default=False, help=\"whether to use TPU.\")\n",
        "flags.DEFINE_integer(\"num_hosts\", default=1, help=\"How many TPU hosts.\")\n",
        "flags.DEFINE_integer(\"num_core_per_host\", default=8,\n",
        "      help=\"8 for TPU v2 and v3-8, 16 for larger TPU v3 pod. In the context \"\n",
        "      \"of GPU training, it refers to the number of GPUs used.\")\n",
        "flags.DEFINE_string(\"tpu_job_name\", default=None, help=\"TPU worker job name.\")\n",
        "flags.DEFINE_string(\"tpu\", default=None, help=\"TPU name.\")\n",
        "flags.DEFINE_string(\"tpu_zone\", default=None, help=\"TPU zone.\")\n",
        "flags.DEFINE_string(\"gcp_project\", default=None, help=\"gcp project.\")\n",
        "flags.DEFINE_string(\"master\", default=None, help=\"master\")\n",
        "flags.DEFINE_integer(\"iterations\", default=1000,\n",
        "                     help=\"number of iterations per TPU training loop.\")\n",
        "\n",
        "# Training\n",
        "flags.DEFINE_bool(\"do_train\", default=True, help=\"whether to do training\")\n",
        "flags.DEFINE_integer(\"train_batch_size\", default=48,\n",
        "                     help=\"batch size for training\")\n",
        "flags.DEFINE_integer(\"train_steps\", default=8000,\n",
        "                     help=\"Number of training steps\")\n",
        "flags.DEFINE_integer(\"warmup_steps\", default=0, help=\"number of warmup steps\")\n",
        "flags.DEFINE_integer(\"save_steps\", default=None,\n",
        "                     help=\"Save the model for every save_steps. \"\n",
        "                     \"If None, not to save any model.\")\n",
        "flags.DEFINE_integer(\"max_save\", default=0,\n",
        "                     help=\"Max number of checkpoints to save. \"\n",
        "                     \"Use 0 to save all.\")\n",
        "flags.DEFINE_integer(\"shuffle_buffer\", default=2048,\n",
        "                     help=\"Buffer size used for shuffle.\")\n",
        "\n",
        "# Optimization\n",
        "flags.DEFINE_float(\"learning_rate\", default=3e-5, help=\"initial learning rate\")\n",
        "flags.DEFINE_float(\"min_lr_ratio\", default=0.0,\n",
        "                   help=\"min lr ratio for cos decay.\")\n",
        "flags.DEFINE_float(\"clip\", default=1.0, help=\"Gradient clipping\")\n",
        "flags.DEFINE_float(\"weight_decay\", default=0.00, help=\"Weight decay rate\")\n",
        "flags.DEFINE_float(\"adam_epsilon\", default=1e-6, help=\"Adam epsilon\")\n",
        "flags.DEFINE_string(\"decay_method\", default=\"poly\", help=\"poly or cos\")\n",
        "flags.DEFINE_float(\"lr_layer_decay_rate\", default=0.75,\n",
        "                   help=\"Top layer: lr[L] = FLAGS.learning_rate.\"\n",
        "                   \"Lower layers: lr[l-1] = lr[l] * lr_layer_decay_rate.\")\n",
        "\n",
        "# Eval / Prediction\n",
        "flags.DEFINE_bool(\"do_predict\", default=False, help=\"whether to do predict\")\n",
        "flags.DEFINE_integer(\"predict_batch_size\", default=32,\n",
        "                     help=\"batch size for prediction\")\n",
        "flags.DEFINE_integer(\"n_best_size\", default=5,\n",
        "                     help=\"n best size for predictions\")\n",
        "flags.DEFINE_integer(\"start_n_top\", default=5, help=\"Beam size for span start.\")\n",
        "flags.DEFINE_integer(\"end_n_top\", default=5, help=\"Beam size for span end.\")\n",
        "flags.DEFINE_string(\"target_eval_key\", default=\"best_f1\",\n",
        "                    help=\"Use has_ans_f1 for Model I.\")\n",
        "flags.DEFINE_string(\"export_dir_base\", default=\"/content/gdrive/MyDrive/HLTC-MRQA/exported-tf-model-1.15.2\",\n",
        "                    help=\"Path of exported models.\")\n",
        "\n",
        "\n",
        "# def del_all_flags(FLAGS):\n",
        "#   flags_dict = FLAGS._flags()\n",
        "#   keys_list = [keys for keys in flags_dict]\n",
        "#   for keys in keys_list:\n",
        "#     FLAGS.delattr(keys)\n",
        "\n",
        "# FLAGS = flags.FLAGS\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "\n",
        "\n",
        "class MRQAExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\n",
        "\n",
        "     For examples without an answer, the start and end position are -1.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               qas_id,\n",
        "               question_text,\n",
        "               paragraph_text,\n",
        "               orig_answer_text=None,\n",
        "               start_position=None,\n",
        "               send_position=None,\n",
        "               is_impossible=False):\n",
        "    self.qas_id = qas_id\n",
        "    self.question_text = question_text\n",
        "    self.paragraph_text = paragraph_text\n",
        "    self.orig_answer_text = orig_answer_text\n",
        "    self.start_position = start_position\n",
        "    self.send_position = send_position\n",
        "    self.is_impossible = is_impossible\n",
        "\n",
        "  def __str__(self):\n",
        "    return self.__repr__()\n",
        "\n",
        "  def __repr__(self):\n",
        "    s = \"\"\n",
        "    s += \"qas_id: %s\" % (printable_text(self.qas_id))\n",
        "    s += \", question_text: %s\" % (\n",
        "        printable_text(self.question_text))\n",
        "    s += \", paragraph_text: [%s]\" % (\" \".join(self.paragraph_text))\n",
        "    if self.start_position:\n",
        "      s += \", start_position: %d\" % (self.start_position)\n",
        "    if self.start_position:\n",
        "      s += \", is_impossible: %r\" % (self.is_impossible)\n",
        "    if self.start_position:\n",
        "      s += \", send_position: %d\" % (self.send_position)\n",
        "    return s\n",
        "\n",
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               unique_id,\n",
        "               example_index,\n",
        "               doc_span_index,\n",
        "               tok_start_to_orig_index,\n",
        "               tok_end_to_orig_index,\n",
        "               token_is_max_context,\n",
        "               input_ids,\n",
        "               input_mask,\n",
        "               p_mask,\n",
        "               segment_ids,\n",
        "               paragraph_len,\n",
        "               cls_index,\n",
        "               start_position=None,\n",
        "               end_position=None,\n",
        "               is_impossible=None):\n",
        "    self.unique_id = unique_id\n",
        "    self.example_index = example_index\n",
        "    self.doc_span_index = doc_span_index\n",
        "    self.tok_start_to_orig_index = tok_start_to_orig_index # paragraph\n",
        "    self.tok_end_to_orig_index = tok_end_to_orig_index     # paragraph\n",
        "    self.token_is_max_context = token_is_max_context\n",
        "    self.input_ids = input_ids                             # context+question\n",
        "    self.input_mask = input_mask\n",
        "    self.p_mask = p_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.paragraph_len = paragraph_len\n",
        "    self.cls_index = cls_index\n",
        "    self.start_position = start_position\n",
        "    self.end_position = end_position\n",
        "    self.is_impossible = is_impossible\n",
        "#########################################################################################Read Test data######################################################################################\n",
        "######################################################################################Read Test data###############################################################################################\n",
        "####################################################################################Read Test data#####################################################################################################\n",
        "def read_mrqa_data(input_file, is_training):\n",
        "  \"\"\"Read a QA data jsonl file into a list of Examples.\"\"\"\n",
        "  with tf.gfile.Open(input_file, \"r\") as reader:\n",
        "    input_data = jsonlines.Reader(reader)\n",
        "\n",
        "  examples = []\n",
        "  for entry in input_data:\n",
        "    if u'header' in entry:\n",
        "      continue\n",
        "\n",
        "    assert type(entry) == dict\n",
        "    assert u'context' in entry\n",
        "    assert u'qas' in entry\n",
        "\n",
        "    paragraph_text = entry[\"context\"]\n",
        "\n",
        "    for qa in entry[\"qas\"]:\n",
        "      assert u'qid' in qa\n",
        "      assert u'question' in qa\n",
        "      assert u'detected_answers' in qa\n",
        "      assert u'text' in qa[u'detected_answers'][0]\n",
        "      assert u'char_spans' in qa[u'detected_answers'][0]\n",
        "\n",
        "      qas_id = qa[\"qid\"]\n",
        "      question_text = qa[\"question\"]\n",
        "      start_position = None\n",
        "      send_position = None\n",
        "      orig_answer_text = None\n",
        "      is_impossible = False\n",
        "      \n",
        "      if is_training:\n",
        "        is_impossible = False if qa[\"detected_answers\"]!=[] else True\n",
        "        # if (len(qa[\"answers\"]) != 1) and (not is_impossible):  # TriviaQA may have several answers, choose the first one\n",
        "        #     raise ValueError(\n",
        "        #         \"For training, each question should have exactly 1 answer.\")\n",
        "        if not is_impossible:\n",
        "          answer = qa[\"detected_answers\"][0]\n",
        "          orig_answer_text = answer[\"text\"]\n",
        "          start_position = answer[\"char_spans\"][0][0]\n",
        "          send_position = answer[\"char_spans\"][0][1]\n",
        "          assert type(start_position)==int\n",
        "        else:\n",
        "          print(qa[\"answers\"])\n",
        "          print(\"=\"*80)\n",
        "          start_position = -1\n",
        "          send_position = -1\n",
        "          orig_answer_text = \"\"        \n",
        "\n",
        "      example = MRQAExample(\n",
        "          qas_id=qas_id,\n",
        "          question_text=question_text,\n",
        "          paragraph_text=paragraph_text,\n",
        "          orig_answer_text=orig_answer_text,\n",
        "          start_position=start_position,\n",
        "          send_position=send_position,\n",
        "          is_impossible=is_impossible)\n",
        "      examples.append(example)\n",
        "  return examples\n",
        "\n",
        "def arrange_mrqa_data(input_data, is_training):\n",
        "  \"\"\"Read a QA data jsonl file into a list of Examples.\"\"\"\n",
        "  examples = []\n",
        "  entry = input_data\n",
        "\n",
        "  assert type(entry) == dict\n",
        "  assert u'context' in entry\n",
        "  assert u'qas' in entry\n",
        "\n",
        "  paragraph_text = entry[\"context\"]\n",
        "  print(input_data)\n",
        "  print(\"=\"*80)\n",
        "  print(len(entry[\"qas\"]))\n",
        "  for qa in entry[\"qas\"]:\n",
        "    assert u'qid' in qa\n",
        "    assert u'question' in qa\n",
        "    assert u'detected_answers' in qa\n",
        "    assert u'text' in qa[u'detected_answers'][0]\n",
        "    assert u'char_spans' in qa[u'detected_answers'][0]\n",
        "\n",
        "    qas_id = qa[\"qid\"]\n",
        "    question_text = qa[\"question\"]\n",
        "    start_position = None\n",
        "    send_position = None\n",
        "    orig_answer_text = None\n",
        "    is_impossible = False\n",
        "    \n",
        "    if is_training:\n",
        "      is_impossible = False if qa[\"detected_answers\"]!=[] else True\n",
        "      # if (len(qa[\"answers\"]) != 1) and (not is_impossible):  # TriviaQA may have several answers, choose the first one\n",
        "      #     raise ValueError(\n",
        "      #         \"For training, each question should have exactly 1 answer.\")\n",
        "      if not is_impossible:\n",
        "        answer = qa[\"detected_answers\"][0]\n",
        "        orig_answer_text = answer[\"text\"]\n",
        "        start_position = answer[\"char_spans\"][0][0]\n",
        "        send_position = answer[\"char_spans\"][0][1]\n",
        "        assert type(start_position)==int\n",
        "      else:\n",
        "        start_position = -1\n",
        "        send_position = -1\n",
        "        orig_answer_text = \"\"        \n",
        "\n",
        "    example = MRQAExample(\n",
        "        qas_id=qas_id,\n",
        "        question_text=question_text,\n",
        "        paragraph_text=paragraph_text,\n",
        "        orig_answer_text=orig_answer_text,\n",
        "        start_position=start_position,\n",
        "        send_position=send_position,\n",
        "        is_impossible=is_impossible)\n",
        "    examples.append(example)\n",
        "    print(len(examples))\n",
        "    print(\"=\"*80)\n",
        "  return examples\n",
        "\n",
        "def _convert_index(index, pos, M=None, is_start=True):\n",
        "  if index[pos] is not None:\n",
        "    return index[pos]\n",
        "  N = len(index)\n",
        "  rear = pos\n",
        "  while rear < N - 1 and index[rear] is None:\n",
        "    rear += 1\n",
        "  front = pos\n",
        "  while front > 0 and index[front] is None:\n",
        "    front -= 1\n",
        "  assert index[front] is not None or index[rear] is not None\n",
        "  if index[front] is None:\n",
        "    if index[rear] >= 1:\n",
        "      if is_start:\n",
        "        return 0\n",
        "      else:\n",
        "        return index[rear] - 1\n",
        "    return index[rear]\n",
        "  if index[rear] is None:\n",
        "    if M is not None and index[front] < M - 1:\n",
        "      if is_start:\n",
        "        return index[front] + 1\n",
        "      else:\n",
        "        return M - 1\n",
        "    return index[front]\n",
        "  if is_start:\n",
        "    if index[rear] > index[front] + 1:\n",
        "      return index[front] + 1\n",
        "    else:\n",
        "      return index[rear]\n",
        "  else:\n",
        "    if index[rear] > index[front] + 1:\n",
        "      return index[rear] - 1\n",
        "    else:\n",
        "      return index[front]\n",
        "\n",
        "def convert_examples_to_features(examples, sp_model, max_seq_length,\n",
        "                                 doc_stride, max_query_length, is_training,\n",
        "                                 output_fn):\n",
        "  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "  cnt_pos, cnt_neg = 0, 0\n",
        "  unique_id = 1000000000\n",
        "  max_N, max_M = 1024, 1024\n",
        "  f = np.zeros((max_N, max_M), dtype=np.float32)\n",
        "\n",
        "  for (example_index, example) in enumerate(examples):\n",
        "\n",
        "    if example_index % 100 == 0:\n",
        "      tf.logging.info('Converting {}/{} pos {} neg {}'.format(\n",
        "          example_index, len(examples), cnt_pos, cnt_neg))\n",
        "\n",
        "    query_tokens = encode_ids(\n",
        "        sp_model,\n",
        "        preprocess_text(example.question_text, lower=FLAGS.uncased))\n",
        "\n",
        "    if len(query_tokens) > max_query_length:\n",
        "      query_tokens = query_tokens[0:max_query_length]\n",
        "\n",
        "    paragraph_text = example.paragraph_text\n",
        "    para_tokens = encode_pieces(\n",
        "        sp_model,\n",
        "        preprocess_text(example.paragraph_text, lower=FLAGS.uncased))\n",
        "\n",
        "    chartok_to_tok_index = []\n",
        "    tok_start_to_chartok_index = []\n",
        "    tok_end_to_chartok_index = []\n",
        "    char_cnt = 0\n",
        "    for i, token in enumerate(para_tokens):\n",
        "      chartok_to_tok_index.extend([i] * len(token))\n",
        "      tok_start_to_chartok_index.append(char_cnt)\n",
        "      char_cnt += len(token)\n",
        "      tok_end_to_chartok_index.append(char_cnt - 1)\n",
        "\n",
        "    tok_cat_text = ''.join(para_tokens).replace(SPIECE_UNDERLINE, ' ')\n",
        "    N, M = len(paragraph_text), len(tok_cat_text)\n",
        "\n",
        "    if N > max_N or M > max_M:\n",
        "      max_N = max(N, max_N)\n",
        "      max_M = max(M, max_M)\n",
        "      f = np.zeros((max_N, max_M), dtype=np.float32)\n",
        "      gc.collect()\n",
        "\n",
        "    g = {}\n",
        "\n",
        "    def _lcs_match(max_dist):\n",
        "      f.fill(0)\n",
        "      g.clear()\n",
        "\n",
        "      ### longest common sub sequence\n",
        "      # f[i, j] = max(f[i - 1, j], f[i, j - 1], f[i - 1, j - 1] + match(i, j))\n",
        "      for i in range(N):\n",
        "\n",
        "        # unlike standard LCS, this is specifically optimized for the setting\n",
        "        # because the mismatch between sentence pieces and original text will\n",
        "        # be small\n",
        "        for j in range(i - max_dist, i + max_dist):\n",
        "          if j >= M or j < 0: continue\n",
        "\n",
        "          if i > 0:\n",
        "            g[(i, j)] = 0\n",
        "            f[i, j] = f[i - 1, j]\n",
        "\n",
        "          if j > 0 and f[i, j - 1] > f[i, j]:\n",
        "            g[(i, j)] = 1\n",
        "            f[i, j] = f[i, j - 1]\n",
        "\n",
        "          f_prev = f[i - 1, j - 1] if i > 0 and j > 0 else 0\n",
        "          if (preprocess_text(paragraph_text[i], lower=FLAGS.uncased,\n",
        "              remove_space=False)\n",
        "              == tok_cat_text[j]\n",
        "              and f_prev + 1 > f[i, j]):\n",
        "            g[(i, j)] = 2\n",
        "            f[i, j] = f_prev + 1\n",
        "\n",
        "    max_dist = abs(N - M) + 5\n",
        "    for _ in range(2):\n",
        "      _lcs_match(max_dist)\n",
        "      if f[N - 1, M - 1] > 0.8 * N: break\n",
        "      max_dist *= 2\n",
        "\n",
        "    orig_to_chartok_index = [None] * N\n",
        "    chartok_to_orig_index = [None] * M\n",
        "    i, j = N - 1, M - 1\n",
        "    while i >= 0 and j >= 0:\n",
        "      if (i, j) not in g: break\n",
        "      if g[(i, j)] == 2:\n",
        "        orig_to_chartok_index[i] = j\n",
        "        chartok_to_orig_index[j] = i\n",
        "        i, j = i - 1, j - 1\n",
        "      elif g[(i, j)] == 1:\n",
        "        j = j - 1\n",
        "      else:\n",
        "        i = i - 1\n",
        "\n",
        "    if all(v is None for v in orig_to_chartok_index) or f[N - 1, M - 1] < 0.8 * N:\n",
        "      print('MISMATCH DETECTED!')\n",
        "      continue\n",
        "\n",
        "    tok_start_to_orig_index = []\n",
        "    tok_end_to_orig_index = []\n",
        "    for i in range(len(para_tokens)):\n",
        "      start_chartok_pos = tok_start_to_chartok_index[i]\n",
        "      end_chartok_pos = tok_end_to_chartok_index[i]\n",
        "      start_orig_pos = _convert_index(chartok_to_orig_index, start_chartok_pos,\n",
        "                                      N, is_start=True)\n",
        "      end_orig_pos = _convert_index(chartok_to_orig_index, end_chartok_pos,\n",
        "                                    N, is_start=False)\n",
        "\n",
        "      tok_start_to_orig_index.append(start_orig_pos)\n",
        "      tok_end_to_orig_index.append(end_orig_pos)\n",
        "\n",
        "    if not is_training:\n",
        "      tok_start_position = tok_end_position = None\n",
        "\n",
        "    if is_training and example.is_impossible:\n",
        "      tok_start_position = -1\n",
        "      tok_end_position = -1\n",
        "\n",
        "    if is_training and not example.is_impossible:\n",
        "      start_position = example.start_position\n",
        "      # end_position = start_position + len(example.orig_answer_text) - 1\n",
        "      end_position = example.send_position\n",
        "\n",
        "      start_chartok_pos = _convert_index(orig_to_chartok_index, start_position,\n",
        "                                         is_start=True)\n",
        "      tok_start_position = chartok_to_tok_index[start_chartok_pos]\n",
        "\n",
        "      end_chartok_pos = _convert_index(orig_to_chartok_index, end_position,\n",
        "                                       is_start=False)\n",
        "      tok_end_position = chartok_to_tok_index[end_chartok_pos]\n",
        "      assert tok_start_position <= tok_end_position\n",
        "\n",
        "    def _piece_to_id(x):\n",
        "      if six.PY2 and isinstance(x, unicode):\n",
        "        x = x.encode('utf-8')\n",
        "      return sp_model.PieceToId(x)\n",
        "\n",
        "    all_doc_tokens = list(map(_piece_to_id, para_tokens))\n",
        "\n",
        "    # The -3 accounts for [CLS], [SEP] and [SEP]\n",
        "    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
        "\n",
        "    # We can have documents that are longer than the maximum sequence length.\n",
        "    # To deal with this we do a sliding window approach, where we take chunks\n",
        "    # of the up to our max length with a stride of `doc_stride`.\n",
        "    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"DocSpan\", [\"start\", \"length\"])\n",
        "    doc_spans = []\n",
        "    start_offset = 0\n",
        "    while start_offset < len(all_doc_tokens):\n",
        "      length = len(all_doc_tokens) - start_offset\n",
        "      if length > max_tokens_for_doc:\n",
        "        length = max_tokens_for_doc\n",
        "      doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
        "      if start_offset + length == len(all_doc_tokens):\n",
        "        break\n",
        "      start_offset += min(length, doc_stride)\n",
        "\n",
        "    for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
        "      tokens = []\n",
        "      token_is_max_context = {}\n",
        "      segment_ids = []\n",
        "      p_mask = []\n",
        "\n",
        "      cur_tok_start_to_orig_index = []\n",
        "      cur_tok_end_to_orig_index = []\n",
        "\n",
        "      for i in range(doc_span.length):\n",
        "        split_token_index = doc_span.start + i\n",
        "\n",
        "        cur_tok_start_to_orig_index.append(\n",
        "            tok_start_to_orig_index[split_token_index])\n",
        "        cur_tok_end_to_orig_index.append(\n",
        "            tok_end_to_orig_index[split_token_index])\n",
        "\n",
        "        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
        "                                               split_token_index)\n",
        "        token_is_max_context[len(tokens)] = is_max_context\n",
        "        tokens.append(all_doc_tokens[split_token_index])\n",
        "        segment_ids.append(SEG_ID_P)\n",
        "        p_mask.append(0)\n",
        "\n",
        "      paragraph_len = len(tokens)\n",
        "\n",
        "      tokens.append(SEP_ID)\n",
        "      segment_ids.append(SEG_ID_P)\n",
        "      p_mask.append(1)\n",
        "\n",
        "      # note(zhiliny): we put P before Q\n",
        "      # because during pretraining, B is always shorter than A\n",
        "      for token in query_tokens:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(SEG_ID_Q)\n",
        "        p_mask.append(1)\n",
        "      tokens.append(SEP_ID)\n",
        "      segment_ids.append(SEG_ID_Q)\n",
        "      p_mask.append(1)\n",
        "\n",
        "      cls_index = len(segment_ids)\n",
        "      tokens.append(CLS_ID)\n",
        "      segment_ids.append(SEG_ID_CLS)\n",
        "      p_mask.append(0)\n",
        "\n",
        "      input_ids = tokens\n",
        "\n",
        "      # The mask has 0 for real tokens and 1 for padding tokens. Only real\n",
        "      # tokens are attended to.\n",
        "      input_mask = [0] * len(input_ids)\n",
        "\n",
        "      # Zero-pad up to the sequence length.\n",
        "      while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(1)\n",
        "        segment_ids.append(SEG_ID_PAD)\n",
        "        p_mask.append(1)\n",
        "\n",
        "      assert len(input_ids) == max_seq_length\n",
        "      assert len(input_mask) == max_seq_length\n",
        "      assert len(segment_ids) == max_seq_length\n",
        "      assert len(p_mask) == max_seq_length\n",
        "\n",
        "      span_is_impossible = example.is_impossible\n",
        "      start_position = None\n",
        "      end_position = None\n",
        "      if is_training and not span_is_impossible:\n",
        "        # For training, if our document chunk does not contain an annotation\n",
        "        # we throw it out, since there is nothing to predict.\n",
        "        doc_start = doc_span.start\n",
        "        doc_end = doc_span.start + doc_span.length - 1\n",
        "        out_of_span = False\n",
        "        if not (tok_start_position >= doc_start and\n",
        "                tok_end_position <= doc_end):\n",
        "          # print(\"out of span\")\n",
        "          # print(\"{}|{}|{}|{}\".format(doc_start,tok_start_position,tok_end_position,doc_end))\n",
        "          out_of_span = True\n",
        "        if out_of_span:\n",
        "          # continue\n",
        "          start_position = 0\n",
        "          end_position = 0\n",
        "          span_is_impossible = True\n",
        "        else:\n",
        "          # note(zhiliny): we put P before Q, so doc_offset should be zero.\n",
        "          # doc_offset = len(query_tokens) + 2\n",
        "          doc_offset = 0\n",
        "          start_position = tok_start_position - doc_start + doc_offset\n",
        "          end_position = tok_end_position - doc_start + doc_offset\n",
        "\n",
        "      if is_training and span_is_impossible:\n",
        "        start_position = cls_index\n",
        "        end_position = cls_index\n",
        "\n",
        "          # note(zhiliny): With multi processing,\n",
        "          # the example_index is actually the index within the current process\n",
        "          # therefore we use example_index=None to avoid being used in the future.\n",
        "          # The current code does not use example_index of training data.\n",
        "      if is_training:\n",
        "        feat_example_index = None\n",
        "      else:\n",
        "        feat_example_index = example_index\n",
        "\n",
        "      feature = InputFeatures(\n",
        "          unique_id=unique_id,\n",
        "          example_index=feat_example_index,\n",
        "          doc_span_index=doc_span_index,\n",
        "          tok_start_to_orig_index=cur_tok_start_to_orig_index,\n",
        "          tok_end_to_orig_index=cur_tok_end_to_orig_index,\n",
        "          token_is_max_context=token_is_max_context,\n",
        "          input_ids=input_ids,\n",
        "          input_mask=input_mask,\n",
        "          p_mask=p_mask,\n",
        "          segment_ids=segment_ids,\n",
        "          paragraph_len=paragraph_len,\n",
        "          cls_index=cls_index,\n",
        "          start_position=start_position,\n",
        "          end_position=end_position,\n",
        "          is_impossible=span_is_impossible)\n",
        "\n",
        "      # Run callback\n",
        "      output_fn(feature)\n",
        "\n",
        "      unique_id += 1\n",
        "      if span_is_impossible:\n",
        "        cnt_neg += 1\n",
        "      else:\n",
        "        cnt_pos += 1\n",
        "\n",
        "  tf.logging.info(\"Total number of instances: {} = pos {} neg {}\".format(\n",
        "      cnt_pos + cnt_neg, cnt_pos, cnt_neg))\n",
        "\t\n",
        "\n",
        "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
        "  \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
        "\n",
        "  # Because of the sliding window approach taken to scoring documents, a single ######################################################################################################################### 512\n",
        "  # token can appear in multiple documents. E.g.\n",
        "  #  Doc: the man went to the store and bought a gallon of milk\n",
        "  #  Span A: the man went to the\n",
        "  #  Span B: to the store and bought\n",
        "  #  Span C: and bought a gallon of\n",
        "  #  ...\n",
        "  #\n",
        "  # Now the word 'bought' will have two scores from spans B and C. We only\n",
        "  # want to consider the score with \"maximum context\", which we define as\n",
        "  # the *minimum* of its left and right context (the *sum* of left and\n",
        "  # right context will always be the same, of course).\n",
        "  #\n",
        "  # In the example the maximum context for 'bought' would be span C since\n",
        "  # it has 1 left context and 3 right context, while span B has 4 left context\n",
        "  # and 0 right context.\n",
        "  best_score = None\n",
        "  best_span_index = None\n",
        "  for (span_index, doc_span) in enumerate(doc_spans):\n",
        "    end = doc_span.start + doc_span.length - 1\n",
        "    if position < doc_span.start:\n",
        "      continue\n",
        "    if position > end:\n",
        "      continue\n",
        "    num_left_context = position - doc_span.start\n",
        "    num_right_context = end - position\n",
        "    score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
        "    if best_score is None or score > best_score:\n",
        "      best_score = score\n",
        "      best_span_index = span_index\n",
        "\n",
        "  return cur_span_index == best_span_index\n",
        "\n",
        "class FeatureWriter(object):\n",
        "  \"\"\"Writes InputFeature to TF example file.\"\"\"\n",
        "\n",
        "  def __init__(self, is_training):\n",
        "    self.is_training = is_training\n",
        "    self.num_features = 0\n",
        "\n",
        "  def process_feature(self, feature):\n",
        "    \"\"\"Write a InputFeature to the TFRecordWriter as a tf.train.Example.\"\"\"\n",
        "    self.num_features += 1\n",
        "\n",
        "    def create_int_feature(values):\n",
        "      feature = tf.train.Feature(\n",
        "          int64_list=tf.train.Int64List(value=list(values)))\n",
        "      return feature\n",
        "\n",
        "    def create_float_feature(values):\n",
        "      f = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))\n",
        "      return f\n",
        "\n",
        "    features = collections.OrderedDict()\n",
        "    features[\"unique_ids\"] = create_int_feature([feature.unique_id])\n",
        "    features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
        "    features[\"input_mask\"] = create_float_feature(feature.input_mask)\n",
        "    features[\"p_mask\"] = create_float_feature(feature.p_mask)\n",
        "    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
        "\n",
        "    features[\"cls_index\"] = create_int_feature([feature.cls_index])\n",
        "\n",
        "    if self.is_training:\n",
        "      features[\"start_positions\"] = create_int_feature([feature.start_position])\n",
        "      features[\"end_positions\"] = create_int_feature([feature.end_position])\n",
        "      impossible = 0\n",
        "      if feature.is_impossible:\n",
        "        impossible = 1\n",
        "      features[\"is_impossible\"] = create_float_feature([impossible])\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "    return tf_example.SerializeToString()\n",
        "\n",
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "    [\"unique_id\", \"start_top_log_probs\", \"start_top_index\",\n",
        "    \"end_top_log_probs\", \"end_top_index\", \"cls_logits\"])\n",
        "\n",
        "_PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "    \"PrelimPrediction\",\n",
        "    [\"feature_index\", \"start_index\", \"end_index\",\n",
        "    \"start_log_prob\", \"end_log_prob\"])\n",
        "\n",
        "_NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "    \"NbestPrediction\", [\"text\", \"start_log_prob\", \"end_log_prob\"])\n",
        "################################################################################## Print Prediction Out ###############################################################################################\n",
        "################################################################################## Print Prediction Out ###############################################################################################\n",
        "################################################################################## Print Prediction Out ###############################################################################################\n",
        "def get_predictions(all_examples, all_features, all_results, n_best_size,\n",
        "                    max_answer_length, lis):\n",
        "  \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
        "  tf.logging.info(\"Getting predictions\")\n",
        "\n",
        "  example_index_to_features = collections.defaultdict(list)\n",
        "  for feature in all_features:\n",
        "    example_index_to_features[feature.example_index].append(feature)\n",
        "\n",
        "  unique_id_to_result = {}\n",
        "  for result in all_results:\n",
        "    unique_id_to_result[result.unique_id] = result\n",
        "\n",
        "  all_predictions = collections.OrderedDict()\n",
        "\n",
        "  for (example_index, example) in enumerate(all_examples):\n",
        "    features = example_index_to_features[example_index]\n",
        "\n",
        "    prelim_predictions = []\n",
        "    # keep track of the minimum score of null start+end of position 0\n",
        "    score_null = 1000000  # large and positive\n",
        "\n",
        "    for (feature_index, feature) in enumerate(features):\n",
        "      result = unique_id_to_result[feature.unique_id]\n",
        "\n",
        "      cur_null_score = result.cls_logits\n",
        "\n",
        "      # if we could have irrelevant answers, get the min score of irrelevant\n",
        "      score_null = min(score_null, cur_null_score)\n",
        "\n",
        "      for i in range(FLAGS.start_n_top):\n",
        "        for j in range(FLAGS.end_n_top):\n",
        "          start_log_prob = result.start_top_log_probs[i]\n",
        "          start_index = result.start_top_index[i]\n",
        "\n",
        "          j_index = i * FLAGS.end_n_top + j\n",
        "\n",
        "          end_log_prob = result.end_top_log_probs[j_index]\n",
        "          end_index = result.end_top_index[j_index]\n",
        "\n",
        "          # We could hypothetically create invalid predictions, e.g., predict\n",
        "          # that the start of the span is in the question. We throw out all\n",
        "          # invalid predictions.\n",
        "          if start_index >= feature.paragraph_len - 1:\n",
        "            continue\n",
        "          if end_index >= feature.paragraph_len - 1:\n",
        "            continue\n",
        "\n",
        "          if not feature.token_is_max_context.get(start_index, False):\n",
        "            continue\n",
        "          if end_index < start_index:\n",
        "            continue\n",
        "          length = end_index - start_index + 1\n",
        "          if length > max_answer_length:\n",
        "            continue\n",
        "\n",
        "          prelim_predictions.append(\n",
        "              _PrelimPrediction(\n",
        "                  feature_index=feature_index,\n",
        "                  start_index=start_index,\n",
        "                  end_index=end_index,\n",
        "                  start_log_prob=start_log_prob,\n",
        "                  end_log_prob=end_log_prob))\n",
        "\n",
        "    prelim_predictions = sorted(\n",
        "        prelim_predictions,\n",
        "        key=lambda x: (x.start_log_prob + x.end_log_prob),\n",
        "        reverse=True)\n",
        "\n",
        "    seen_predictions = {}\n",
        "    nbest = []\n",
        "    for pred in prelim_predictions:\n",
        "      if len(nbest) >= n_best_size:\n",
        "        break\n",
        "      feature = features[pred.feature_index]\n",
        "\n",
        "      tok_start_to_orig_index = feature.tok_start_to_orig_index\n",
        "      tok_end_to_orig_index = feature.tok_end_to_orig_index\n",
        "      start_orig_pos = tok_start_to_orig_index[pred.start_index]\n",
        "      end_orig_pos = tok_end_to_orig_index[pred.end_index]\n",
        "\n",
        "      paragraph_text = example.paragraph_text\n",
        "      final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip()\n",
        "\n",
        "      if final_text in seen_predictions:\n",
        "        continue\n",
        "\n",
        "      seen_predictions[final_text] = True\n",
        "\n",
        "      nbest.append(\n",
        "          _NbestPrediction(\n",
        "              text=final_text,\n",
        "              start_log_prob=pred.start_log_prob,\n",
        "              end_log_prob=pred.end_log_prob))\n",
        " ################# Add top 5 ####################  \n",
        "    all_pred = []\n",
        "    s_t = []\n",
        "    e_t = []\n",
        "    for pred in prelim_predictions:\n",
        "  \n",
        "      feature = features[pred.feature_index]\n",
        "\n",
        "      tok_start_to_orig_index = feature.tok_start_to_orig_index\n",
        "      tok_end_to_orig_index = feature.tok_end_to_orig_index\n",
        "      start_orig_pos = tok_start_to_orig_index[pred.start_index]\n",
        "      end_orig_pos = tok_end_to_orig_index[pred.end_index]\n",
        "\n",
        "      paragraph_text = example.paragraph_text\n",
        "      final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip()\n",
        "\n",
        "      xx = pred.start_log_prob\n",
        "      yy = pred.end_log_prob\n",
        "\n",
        "      j  = xx + yy\n",
        "\n",
        "      all_pred.append([final_text,j])\n",
        "\n",
        "      # all_pred.append([pred])\n",
        "      # all_pred.append(final_text)   ## give all the selected spans\n",
        "      # all_pred.append([start_orig_pos,end_orig_pos])    ## give start and end token_num for all the selected spans\n",
        "      s_t.append(start_orig_pos)\n",
        "      e_t.append(end_orig_pos)\n",
        "    lis.append(all_pred)\n",
        "\n",
        "    # all_pred.append([min(s_t),max(e_t)])  ## give min start and max end token_num for all the selected spans to get one span\n",
        "\n",
        "    # start_orig_pos_min = min(s_t)\n",
        "    # end_orig_pos_max = max(e_t)\n",
        "    # paragraph_text = example.paragraph_text  ## give one max span span\n",
        "    # final_max_text = paragraph_text[start_orig_pos_min: end_orig_pos_max + 1].strip()\n",
        "    # j = [start_orig_pos_min, end_orig_pos_max]\n",
        "\n",
        "    # all_pred.append([j,final_max_text,])\n",
        "\n",
        "\n",
        "################# Add top 5 ####################        \n",
        "    # In very rare edge cases we could have no valid predictions. So we\n",
        "    # just create a nonce prediction in this case to avoid failure.\n",
        "    if not nbest:\n",
        "      nbest.append(\n",
        "          _NbestPrediction(text=\"\", start_log_prob=-1e6,\n",
        "          end_log_prob=-1e6))\n",
        "\n",
        "    total_scores = []\n",
        "    best_non_null_entry = None\n",
        "    for entry in nbest:\n",
        "      total_scores.append(entry.start_log_prob + entry.end_log_prob)\n",
        "      if not best_non_null_entry:\n",
        "        best_non_null_entry = entry\n",
        "\n",
        "    assert best_non_null_entry is not None\n",
        "\n",
        "    all_predictions[example.qas_id] = best_non_null_entry.text\n",
        "\n",
        "  # return all_predictions, all_pred\n",
        "  return all_predictions, lis\n",
        "\n",
        "def _get_best_indexes(logits, n_best_size):\n",
        "  \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "  index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  best_indexes = []\n",
        "  for i in range(len(index_and_score)):\n",
        "    if i >= n_best_size:\n",
        "      break\n",
        "    best_indexes.append(index_and_score[i][0])\n",
        "  return best_indexes\n",
        "\n",
        "\n",
        "def _compute_softmax(scores):\n",
        "  \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
        "  if not scores:\n",
        "    return []\n",
        "\n",
        "  max_score = None\n",
        "  for score in scores:\n",
        "    if max_score is None or score > max_score:\n",
        "      max_score = score\n",
        "\n",
        "  exp_scores = []\n",
        "  total_sum = 0.0\n",
        "  for score in scores:\n",
        "    x = math.exp(score - max_score)\n",
        "    exp_scores.append(x)\n",
        "    total_sum += x\n",
        "\n",
        "  probs = []\n",
        "  for score in exp_scores:\n",
        "    probs.append(score / total_sum)\n",
        "  return probs\n",
        "\n",
        "\n",
        "def input_fn_builder(input_glob, seq_length, is_training, drop_remainder,\n",
        "                     num_hosts, num_threads=8):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  name_to_features = {\n",
        "      \"unique_ids\": tf.FixedLenFeature([], tf.int64),\n",
        "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.float32),\n",
        "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"cls_index\": tf.FixedLenFeature([], tf.int64),\n",
        "      \"p_mask\": tf.FixedLenFeature([seq_length], tf.float32)\n",
        "  }\n",
        "\n",
        "  if is_training:\n",
        "    name_to_features[\"start_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
        "    name_to_features[\"end_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
        "    name_to_features[\"is_impossible\"] = tf.FixedLenFeature([], tf.float32)\n",
        "\n",
        "  tf.logging.info(\"Input tfrecord file glob {}\".format(input_glob))\n",
        "  global_input_paths = tf.gfile.Glob(input_glob)\n",
        "  tf.logging.info(\"Find {} input paths {}\".format(\n",
        "      len(global_input_paths), global_input_paths))\n",
        "\n",
        "  def _decode_record(record, name_to_features):\n",
        "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "    example = tf.parse_single_example(record, name_to_features)\n",
        "\n",
        "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "    # So cast all int64 to int32.\n",
        "    for name in list(example.keys()):\n",
        "      t = example[name]\n",
        "      if t.dtype == tf.int64:\n",
        "        t = tf.cast(t, tf.int32)\n",
        "      example[name] = t\n",
        "\n",
        "    return example\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    if FLAGS.use_tpu:\n",
        "      batch_size = params[\"batch_size\"]\n",
        "    elif is_training:\n",
        "      batch_size = FLAGS.train_batch_size\n",
        "    else:\n",
        "      batch_size = FLAGS.predict_batch_size\n",
        "\n",
        "    # Split tfrecords across hosts\n",
        "    if num_hosts > 1:\n",
        "      host_id = params[\"context\"].current_host\n",
        "      num_files = len(global_input_paths)\n",
        "      if num_files >= num_hosts:\n",
        "        num_files_per_host = (num_files + num_hosts - 1) // num_hosts\n",
        "        my_start_file_id = host_id * num_files_per_host\n",
        "        my_end_file_id = min((host_id + 1) * num_files_per_host, num_files)\n",
        "        input_paths = global_input_paths[my_start_file_id: my_end_file_id]\n",
        "      tf.logging.info(\"Host {} handles {} files\".format(host_id,\n",
        "                                                        len(input_paths)))\n",
        "    else:\n",
        "      input_paths = global_input_paths\n",
        "\n",
        "    if len(input_paths) == 1:\n",
        "      d = tf.data.TFRecordDataset(input_paths[0])\n",
        "      # For training, we want a lot of parallel reading and shuffling.\n",
        "      # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "      if is_training:\n",
        "        d = d.shuffle(buffer_size=FLAGS.shuffle_buffer)\n",
        "        d = d.repeat()\n",
        "    else:\n",
        "      d = tf.data.Dataset.from_tensor_slices(input_paths)\n",
        "      # file level shuffle\n",
        "      d = d.shuffle(len(input_paths)).repeat()\n",
        "\n",
        "      # `cycle_length` is the number of parallel files that get read.\n",
        "      cycle_length = min(num_threads, len(input_paths))\n",
        "\n",
        "      d = d.apply(\n",
        "          tf.contrib.data.parallel_interleave(\n",
        "              tf.data.TFRecordDataset,\n",
        "              sloppy=is_training,\n",
        "              cycle_length=cycle_length))\n",
        "\n",
        "      if is_training:\n",
        "        # sample level shuffle\n",
        "        d = d.shuffle(buffer_size=FLAGS.shuffle_buffer)\n",
        "\n",
        "    d = d.apply(\n",
        "        tf.contrib.data.map_and_batch(\n",
        "            lambda record: _decode_record(record, name_to_features),\n",
        "            batch_size=batch_size,\n",
        "            num_parallel_batches=num_threads,\n",
        "            drop_remainder=drop_remainder))\n",
        "    d = d.prefetch(1024)\n",
        "\n",
        "    return d\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "def get_model_fn():\n",
        "  def model_fn(features, labels, mode, params):\n",
        "    #### Training or Evaluation\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    #### Get loss from inputs\n",
        "    outputs = function_builder.get_qa_outputs(FLAGS, features, is_training)\n",
        "\n",
        "    #### Check model parameters\n",
        "    num_params = sum([np.prod(v.shape) for v in tf.trainable_variables()])\n",
        "    tf.logging.info('#params: {}'.format(num_params))\n",
        "\n",
        "    scaffold_fn = None\n",
        "\n",
        "    #### Evaluation mode\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      if FLAGS.init_checkpoint:\n",
        "        tf.logging.info(\"init_checkpoint not being used in predict mode.\")\n",
        "\n",
        "      predictions = {\n",
        "          \"unique_ids\": features[\"unique_ids\"],\n",
        "          \"start_top_index\": outputs[\"start_top_index\"],\n",
        "          \"start_top_log_probs\": outputs[\"start_top_log_probs\"],\n",
        "          \"end_top_index\": outputs[\"end_top_index\"],\n",
        "          \"end_top_log_probs\": outputs[\"end_top_log_probs\"],\n",
        "          \"cls_logits\": outputs[\"cls_logits\"]\n",
        "      }\n",
        "\n",
        "      if FLAGS.use_tpu:\n",
        "        output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "            mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
        "      else:\n",
        "        output_spec = tf.estimator.EstimatorSpec(\n",
        "            mode=mode, predictions=predictions)\n",
        "      return output_spec\n",
        "\n",
        "    ### Compute loss\n",
        "    seq_length = tf.shape(features[\"input_ids\"])[1]\n",
        "    def compute_loss(log_probs, positions):\n",
        "      one_hot_positions = tf.one_hot(positions, depth=seq_length, dtype=tf.float32)\n",
        "\n",
        "      loss = - tf.reduce_sum(one_hot_positions * log_probs, axis=-1)\n",
        "      loss = tf.reduce_mean(loss)\n",
        "      return loss\n",
        "\n",
        "    start_loss = compute_loss(\n",
        "        outputs[\"start_log_probs\"], features[\"start_positions\"])\n",
        "    end_loss = compute_loss(\n",
        "        outputs[\"end_log_probs\"], features[\"end_positions\"])\n",
        "\n",
        "    total_loss = (start_loss + end_loss) * 0.5\n",
        "\n",
        "    cls_logits = outputs[\"cls_logits\"] ## answerability loss\n",
        "    is_impossible = tf.reshape(features[\"is_impossible\"], [-1])\n",
        "    regression_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "        labels=is_impossible, logits=cls_logits)\n",
        "    regression_loss = tf.reduce_mean(regression_loss)\n",
        "\n",
        "    # note(zhiliny): by default multiply the loss by 0.5 so that the scale is\n",
        "    # comparable to start_loss and end_loss\n",
        "    total_loss += regression_loss * 0.5\n",
        "\n",
        "    #### Configuring the optimizer\n",
        "    train_op, learning_rate, _ = model_utils.get_train_op(FLAGS, total_loss)\n",
        "\n",
        "    monitor_dict = {}\n",
        "    monitor_dict[\"lr\"] = learning_rate\n",
        "\n",
        "    #### load pretrained models\n",
        "    scaffold_fn = model_utils.init_from_checkpoint(FLAGS)\n",
        "\n",
        "    #### Constucting training TPUEstimatorSpec with new cache.\n",
        "    if FLAGS.use_tpu:\n",
        "      host_call = function_builder.construct_scalar_host_call(\n",
        "          monitor_dict=monitor_dict,\n",
        "          model_dir=FLAGS.model_dir,\n",
        "          prefix=\"train/\",\n",
        "          reduce_fn=tf.reduce_mean)\n",
        "\n",
        "      train_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode, loss=total_loss, train_op=train_op, host_call=host_call,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    else:\n",
        "      train_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode, loss=total_loss, train_op=train_op)\n",
        "\n",
        "    return train_spec\n",
        "\n",
        "  return model_fn\n",
        "\n",
        "def _get_spm_basename():\n",
        "  spm_basename = os.path.basename(FLAGS.spiece_model_file)\n",
        "  return spm_basename\n",
        "\n",
        "################################################################################################# predictor ############################################################################################################################################################################\n",
        "\n",
        "\n",
        "def mrqa_predictor(FLAGS, json_data):\n",
        "  \"\"\"\n",
        "  Get prediction with the data got fron mrqa official request.\n",
        "  \"\"\"\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "  sp_model = spm.SentencePieceProcessor()\n",
        "  sp_model.Load(FLAGS.spiece_model_file)\n",
        "\n",
        "  ### TPU Configuration\n",
        "  run_config = model_utils.configure_tpu(FLAGS)\n",
        "\n",
        "  model_fn = get_model_fn()  # model configs, load the trained model\n",
        "  spm_basename = _get_spm_basename()\n",
        "\n",
        "  tf.logging.info(\"Got Data from Server...\")\n",
        "  eval_data = arrange_mrqa_data(json_data, is_training=False)\n",
        "\n",
        "  eval_writer = FeatureWriter(is_training=False)\n",
        "  eval_features = []\n",
        "  eval_features_inp = []\n",
        "\n",
        "  def append_feature(feature):\n",
        "    eval_features.append(feature)\n",
        "    eval_features_inp.append(eval_writer.process_feature(feature))\n",
        "\n",
        "  convert_examples_to_features(\n",
        "\t\t\texamples=eval_data,\n",
        "\t\t\tsp_model=sp_model,\n",
        "\t\t\tmax_seq_length=FLAGS.max_seq_length,\n",
        "\t\t\tdoc_stride=FLAGS.doc_stride,\n",
        "\t\t\tmax_query_length=FLAGS.max_query_length,\n",
        "\t\t\tis_training=False,\n",
        "\t\t\toutput_fn=append_feature) \n",
        "\n",
        "  predict_fn = tf.contrib.predictor.from_saved_model(FLAGS.export_dir_base)\n",
        "\n",
        "  cur_results = []\n",
        "\n",
        "  for num, eval_feature in enumerate(eval_features_inp):\n",
        "    result = predict_fn({\"examples\":[eval_feature]})\n",
        "\n",
        "    if len(cur_results) % 1000 == 0:\n",
        "      tf.logging.info(\"Processing example: %d\" % (len(cur_results)))\n",
        "\n",
        "    unique_id = int(result[\"unique_ids\"])\n",
        "    start_top_log_probs = (\n",
        "\t\t\t\t[float(x) for x in result[\"start_top_log_probs\"].flat])\n",
        "    start_top_index = [int(x) for x in result[\"start_top_index\"].flat]\n",
        "    end_top_log_probs = (\n",
        "\t\t\t\t[float(x) for x in result[\"end_top_log_probs\"].flat])\n",
        "    end_top_index = [int(x) for x in result[\"end_top_index\"].flat]\n",
        "\n",
        "    cls_logits = float(result[\"cls_logits\"].flat[0])\n",
        "\n",
        "    cur_results.append(\n",
        "\t\t\t\tRawResult(\n",
        "\t\t\t\t\t\tunique_id=unique_id,\n",
        "\t\t\t\t\t\tstart_top_log_probs=start_top_log_probs,\n",
        "\t\t\t\t\t\tstart_top_index=start_top_index,\n",
        "\t\t\t\t\t\tend_top_log_probs=end_top_log_probs,\n",
        "\t\t\t\t\t\tend_top_index=end_top_index,\n",
        "\t\t\t\t\t\tcls_logits=cls_logits))\n",
        "\n",
        "  ret = get_predictions(eval_data, eval_features, cur_results,\n",
        "                        FLAGS.n_best_size, FLAGS.max_answer_length)\n",
        "  print(ret)   ################################################################################### Print the output of the final result ############################################################\n",
        "  return dict(ret)\n",
        "################################################# ######################## after read_mrqa_data, model come here to predictor ###################################################################################################################\n",
        "################################################# predictor ###################################################################################################################\n",
        "def mrqa_tester():\n",
        "  \"\"\"\n",
        "  Get prediction with the data got fron mrqa official request.\n",
        "  \"\"\"\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "  sp_model = spm.SentencePieceProcessor()\n",
        "  sp_model.Load(FLAGS.spiece_model_file)\n",
        "\n",
        "  ### TPU Configuration\n",
        "  run_config = model_utils.configure_tpu(FLAGS)\n",
        "\n",
        "  model_fn = get_model_fn()  # model configs, load the trained model\n",
        "  spm_basename = _get_spm_basename()\n",
        "\n",
        "  dev_file = FLAGS.dev_dir+\"/DROP-piece.jsonl\" ########################################## The Input JSON\n",
        "  tf.logging.info(\"Read data from {}\".format(dev_file))\n",
        "  eval_data = read_mrqa_data(dev_file, is_training=False)\n",
        "\n",
        "  eval_writer = FeatureWriter(is_training=False)\n",
        "  eval_features = []\n",
        "  eval_features_inp = []\n",
        "\n",
        "  def append_feature(feature):\n",
        "    eval_features.append(feature)\n",
        "    eval_features_inp.append(eval_writer.process_feature(feature))\n",
        "\n",
        "  convert_examples_to_features(\n",
        "\t\t\texamples=eval_data,\n",
        "\t\t\tsp_model=sp_model,\n",
        "\t\t\tmax_seq_length=FLAGS.max_seq_length,\n",
        "\t\t\tdoc_stride=FLAGS.doc_stride,\n",
        "\t\t\tmax_query_length=FLAGS.max_query_length,\n",
        "\t\t\tis_training=False,\n",
        "\t\t\toutput_fn=append_feature) \n",
        "\n",
        "  predict_fn = tf.contrib.predictor.from_saved_model(FLAGS.export_dir_base)\n",
        "\n",
        "  cur_results = []\n",
        "  all__ = []\n",
        "  listt = all__\n",
        "  N = 0\n",
        "  for num, eval_feature in enumerate(eval_features_inp):\n",
        "    result = predict_fn({\"examples\":[eval_feature]})\n",
        "\n",
        "    if len(cur_results) % 1000 == 0:\n",
        "      tf.logging.info(\"Processing example: %d\" % (len(cur_results)))\n",
        "\n",
        "    unique_id = int(result[\"unique_ids\"])\n",
        "    start_top_log_probs = (\n",
        "\t\t\t\t[float(x) for x in result[\"start_top_log_probs\"].flat])\n",
        "    start_top_index = [int(x) for x in result[\"start_top_index\"].flat]\n",
        "    end_top_log_probs = (\n",
        "\t\t\t\t[float(x) for x in result[\"end_top_log_probs\"].flat])\n",
        "    end_top_index = [int(x) for x in result[\"end_top_index\"].flat]\n",
        "\n",
        "    cls_logits = float(result[\"cls_logits\"].flat[0])\n",
        "\n",
        "    cur_results.append(\n",
        "\t\t\t\tRawResult(\n",
        "\t\t\t\t\t\tunique_id=unique_id,\n",
        "\t\t\t\t\t\tstart_top_log_probs=start_top_log_probs,\n",
        "\t\t\t\t\t\tstart_top_index=start_top_index,\n",
        "\t\t\t\t\t\tend_top_log_probs=end_top_log_probs,\n",
        "\t\t\t\t\t\tend_top_index=end_top_index,\n",
        "\t\t\t\t\t\tcls_logits=cls_logits))\n",
        "\n",
        "  ret, aaa = get_predictions(eval_data, eval_features, cur_results,\n",
        "                        FLAGS.n_best_size, FLAGS.max_answer_length, listt)\n",
        "  print(aaa)\n",
        "  # all__.append(aaa)\n",
        "\n",
        "  return dict(ret), aaa                                                         ####### Return the answers here ########\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################################FINAL RESULT PRINT HERE############################################################################################################\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#   # del_all_flags(FLAGS)\n",
        "#   # FLAGS(sys.argv)\n",
        "#   mrqa_tester()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgNByMYaWYjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e27584-d166-449c-f71e-09d29e7241c6"
      },
      "source": [
        "# dict with qid and top answers\n",
        "# tf.app.flags.DEFINE_string('f', '', 'kernel') # for first time keep this (ig only in the colab), then comment this line(to remove dubplicate f).\n",
        "reader_out, hmm = mrqa_tester()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /replica:0/task:0/device:GPU:2,/replica:0/task:0/device:GPU:6,/replica:0/task:0/device:GPU:3,/replica:0/task:0/device:GPU:4,/replica:0/task:0/device:GPU:5,/replica:0/task:0/device:GPU:0,/replica:0/task:0/device:GPU:7,/replica:0/task:0/device:GPU:1\n",
            "INFO:tensorflow:Use MirroredStrategy with 8 devices.\n",
            "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Read data from /content/data/dev/DROP-piece.jsonl\n",
            "INFO:tensorflow:Converting 0/10 pos 0 neg 0\n",
            "INFO:tensorflow:Total number of instances: 19 = pos 19 neg 0\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/MyDrive/HLTC-MRQA/exported-tf-model-1.15.2/variables/variables\n",
            "INFO:tensorflow:Processing example: 0\n",
            "INFO:tensorflow:Getting predictions\n",
            "[[['Zoom?', -9.047911822795868], [\"Zoom? So I've heard that's there for a long time\", -11.476564645767212], ['meat', -11.548356056213379], ['meat and they announced the neat bar and the neat board which is a touch display and then Paul Lee', -12.239153861999512], ['meat and they announced the neat bar and the neat board', -12.789748191833496], ['Zoom', -12.792715072631836], [\"meat and they announced the neat bar and the neat board which is a touch display and then Paul Lee announced. Their Appliance x-series. So there is an x 30 which is for smaller rooms. And then there's an x 54 larger rooms\", -13.159703254699707], [\"meat and they announced the neat bar and the neat board which is a touch display and then Paul Lee announced. Their Appliance x-series. So there is an x 30 which is for smaller rooms. And then there's an x 54\", -13.182027816772461]], [['Not much text', -6.257963299751282], ['Not much text absolutely', -7.6872111558914185], ['Not much text absolutely.', -8.112068176269531], ['Not much', -9.247757911682129], ['text', -9.61880373954773], ['native control', -10.15002429485321], ['text absolutely', -11.084724187850952], ['native', -11.391365051269531], ['native control integration.', -11.546382904052734], ['native control integration. How much can the going to be customized? Okay, we talked about that you are going to be using IP Control. So that will be being bailed out more and more. Correct. - (27:28', -12.503212928771973], ['Not', -12.71066427230835], ['text absolutely.', -12.826075315475464]], [['awesome', -9.76253005862236], ['consumer experience', -10.302623867988586], ['Melissa', -10.597243785858154], ['IP based', -10.716168880462646], ['consumer experience just like you would set up a display', -11.411467552185059], ['consumer experience just like you would set up a display with a sound system', -11.42758560180664], ['IP', -11.515376567840576], ['IP based as long as they can I pee', -11.60387659072876], ['consumer', -11.683674812316895], ['consumer experience just like you would set up a display with a sound system at home', -11.810742378234863], ['firmware', -12.378593444824219], ['awesome you covered a lot but you know what you left out.', -12.607691526412964], ['IP based as long as they can I pee command. Yep. All right kids Optical', -12.677516460418701], ['awesome you covered a lot', -12.943442106246948], ['awesome you covered a lot but you know what you left out. - (20:13)  SPEAKER: Esther Yoon\\nWhat the phone', -13.000631093978882]], [['carbon footprint', -9.92733883857727], ['carbon footprint', -10.246058642864227], ['LED Wireless belt', -10.469384551048279], ['two', -10.695582866668701], ['LED Wireless', -10.805814146995544], ['carbon footprint hmm in talking about that reduction within the carbon footprint', -10.818270921707153], ['LED Wireless belt on owes a debt that we controlled from the zoo', -11.043057799339294], ['what all is new with zoom', -11.071001291275024], ['LED', -11.258073329925537], ['two Rockstar kind of moments, but let me get to what what all of our guests want to know. So I see a couple of questions like well, yeah, what all is new with zoom', -11.387782573699951], ['carbon', -11.891735315322876], [\"carbon footprint because we can do everything via video. Yep. You don't have to travel the miles.\", -12.331176042556763], ['what all is new', -12.419607400894165], [\"two Rockstar kind of moments, but let me get to what what all of our guests want to know. So I see a couple of questions like well, yeah, what all is new with zoom and what we can offer to our customers. I think we've covered a lot\", -12.742032527923584], [\"what all is new with zoom and what we can offer to our customers. I think we've covered a lot of that\", -12.98731541633606], ['what all is new with zoom and', -13.143167734146118], ['carbon footprint hmm in talking about that reduction', -13.203859567642212], ['carbon footprint hmm in talking about that reduction within the carbon footprint because we can do everything via video. Yep.', -14.054760932922363], ['carbon', -14.113659858703613], [\"carbon footprint because we can do everything via video. Yep. You don't have to travel the miles. I hate to fly. So anytime I can do it via video.\", -14.142250061035156]], [['90%', -4.601068966090679], ['about 90%', -5.519722282886505], ['wherever they are.', -5.555246517062187], ['right', -6.315600872039795], ['about 90% of use cases', -6.4200193881988525], ['drag this wherever they are.', -6.572456955909729], ['drag', -6.920649170875549], [\"Yep, and that's really great because if you think about it between these two partners you have your Sighs or sorry small conference rooms with the x30 midsize conference rooms with the x 50 and if you want to zoom rooms for touch you have the d-10 so it hits about 90% of use cases\", -7.180545330047607], [\"Yep, and that's really great\", -7.1822190284729], ['drag this', -7.536836266517639], ['about 90% of', -7.581336259841919], ['90% of use cases', -7.825608730316162], [\"Yep, and that's really great because if you think about it between these two partners you have your Sighs or sorry small conference rooms with the x30 midsize conference rooms with the x 50 and if you want to zoom rooms for touch you have the d-10 so it hits about 90% of use cases from what I've seen.\", -8.127978801727295], ['90% of', -8.6107497215271], ['wherever they are. And so essentially once that room is assigned to a zoom room.', -8.774301767349243], [\"about 90% of use cases from what I've seen.\", -8.849979639053345], ['Yep,', -8.969591617584229], [\"Yep, and that's really great because if you think about it between these two partners you have your Sighs or sorry small conference rooms with the x30 midsize conference rooms with the x 50 and if you want to zoom rooms for touch you have the d-10 so it hits about 90% of use cases from\", -8.977296352386475], [\"90% of use cases from what I've seen.\", -9.266808032989502]], [['Can we customize?', -5.7473345175385475], ['simultaneous interpretation. Yep. So, okay. It left us all scratching our heads. How does that work? How much of the GUI? Can we customize?', -7.418205499649048], ['simultaneous interpretation.', -7.689653635025024], ['different modalities of a different of a control system', -8.475893437862396], ['different modalities', -8.798646450042725], ['simultaneous', -8.827630281448364], ['Can we customize', -9.269689559936523], ['simultaneous interpretation', -9.428966760635376], ['different modalities of a different of a control system that are preset.', -10.982629299163818], ['different modalities of a different of a control system that are preset', -11.192640781402588], [\"Can we customize? It's a great question.\", -11.239940166473389], ['different modalities of a different of a control', -11.711997509002686]], [['meeting', -3.6630749379401095], ['meeting', -4.0998002253472805], ['into the meeting', -4.120570285245776], ['get it to a meeting', -4.398133777081966], ['get it to a meeting and', -9.296708583831787], ['get it', -9.394779205322266], ['into', -10.676473140716553], ['into the', -11.78748893737793], ['into the meeting and', -11.833650588989258], [\"into the meeting and I'm like, we're already there.\", -12.383305549621582], [\"meeting and I'm like, we're already there.\", -13.887256860733032], ['meeting and', -14.0686514377594]], [['you can have remote', -5.734853982925415], ['now with our Zoom rooms appliances you can have remote', -6.050734639167786], ['now with our Zoom rooms appliances', -6.77375328540802], ['You can have a lockdown OS designed to only run', -6.850412368774414], ['You can have a lockdown OS designed to only run Zoom rooms.', -6.857361793518066], ['you can have remote for more updates', -6.881643533706665], ['You can have a lockdown OS', -6.985501289367676], ['you can have remote for more updates for all the peripherals on the appliances.', -6.990528345108032], ['But now with our Zoom rooms appliances you can have remote', -7.262768507003784], ['But now with our Zoom rooms appliances you can have remote for more updates for all the peripherals on the appliances.', -7.4843785762786865], ['now', -7.614337921142578], ['But now with our Zoom rooms appliances', -7.664271116256714], ['You can have a lockdown OS designed to only run Zoom rooms. And then you can have that Global streamlined Hardware procurement option,', -7.670332908630371], ['now with our Zoom rooms appliances you can have remote for more updates for all the peripherals on the appliances.', -7.760952949523926], ['now with our Zoom rooms appliances you can have remote for more updates', -7.968855857849121], ['But now with our Zoom rooms appliances you can have remote for more updates', -8.420095205307007], ['You can have a lockdown', -8.606346130371094], ['But now with our Zoom rooms appliances you can have remote for more updates for all the peripherals', -8.7790048122406]], [[\"you're able to have a virtual background\", -7.321069121360779], [\"listen to you and translate what you're saying into a interpreter Channel\", -7.32358056306839], [\"you're able to have a virtual background for your Zoom room.\", -8.031900763511658], ['listen to you and translate', -8.097331762313843], [\"you're able to have a virtual background for your\", -8.344749808311462], [\"Hey, I want to hear the Italian translation of what Bobby saying I could find it if there's an interpreter signed\", -8.871323585510254], [\"I could find it if there's an interpreter signed and what happens is Bobby's voice would go down to 20% and I would hear my interpreter voice at 80%\", -9.016065001487732], [\"I could find it if there's an interpreter signed\", -9.279993414878845], ['Hey, I want to hear the Italian translation of what Bobby', -9.297881126403809], [\"I could find it if there's an interpreter\", -9.33334493637085], ['Hey, I want to hear the Italian translation of what Bobby saying', -9.355152130126953], ['Hey, I want to hear the Italian translation', -9.454768180847168], [\"Hey, I want to hear the Italian translation of what Bobby saying I could find it if there's an interpreter\", -9.59704303741455], ['listen', -9.624783277511597], ['listen to you', -9.729173421859741]], [['Esther Yoon', -1.15748560288921], ['marketing', -4.478865519165993], ['conference room Solutions', -5.04610363394022], ['product marketing', -5.678847908973694], ['product marketing manager', -5.94415819644928], ['marketing manager', -6.4505109786987305], ['Esther', -6.504528045654297], ['Sam Cooke Keiko', -7.432068288326263], ['Sam Cooke', -7.672475278377533], ['product marketing manager for conference room Solutions', -8.337467670440674], ['Esther Yoon who is the product marketing', -8.355233192443848], ['training', -8.664039179682732], ['Esther Yoon who is the product marketing manager', -9.045952796936035], ['marketing manager for conference room Solutions', -9.267640113830566], ['product', -9.336512088775635], ['conference room', -9.375620365142822], ['conference room Solutions joining us today', -9.48277473449707], ['Esther Yoon who is the product marketing manager for conference room Solutions', -9.628448009490967], ['conference room Solutions joining us', -10.129884243011475], ['working', -12.459805488586426], [\"Sam Cooke Keiko who's on our PSO\", -13.971820831298828], [\"Sam Cooke Keiko who's on our PSO team\", -14.23678731918335], ['Sam', -14.702472686767578]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_dRPfIdeyMD"
      },
      "source": [
        "w  = [4,12,43.3,19,100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TmZXtYmen31",
        "outputId": "d0fd3376-e464-424d-c5fb-db69d95993a1"
      },
      "source": [
        "print(max(w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdoLDprOWXgv"
      },
      "source": [
        "# Answer 1\n",
        "s = ['90%', 'carbon footprint', 'extra', 'carbon footprint', '80%', 'about 90%', 'carbon', '20%', 'extra cost', 'about 90% of use cases', 'like an extra cost.', 'an extra cost', 'an extra cost.', 'like an extra cost', 'cost conscious', '90% of use cases', 'footprint', 'scheduling', 'extra cost.', 'carbon', 'drive a video', '20% and I would hear my interpreter voice at 80%', 'drive a video first experience', '$50', 'they were built from the ground up all the way down to the OS', 'like an extra', 'an extra', 'reduction', 'video', 'g', 'collaboration and communication', 'collaboration and communication should be seamless, even when the needs for collaboration communication', \"add participants. So it's added to their calendar and there's a I'm just going to Breeze through it and then a visitor it preserves the room\", 'collaboration', 'about 90% of', 'she is a bit of a rock star when it comes to being able to get things done', 'she is a bit of a rock star', 'communication', '$50 gift card', \"add participants. So it's added to their calendar\", 'native control', 'reloading', 'drive', 'they were built from the ground up', 'focus room', 'okay on time', 'focus', 'success is really going to be tied to our customer success', 'video first experience', '90% of', 'cost', \"corporate background image, but I imagine that probably comes soon. \\n\\n- (26:02)  SPEAKER: Esther Yoon\\nYeah, I wouldn't get out for sure because it's something that people have asked for that, you know branding is a huge part of large Enterprises\", 'reduction within the carbon footprint', 'zoom platform', 'she is a bit of a rock star when it comes to being able to get things done being able to answer questions.', 'conference room Solutions', 'drive a video first', \"success is inherently tied together, right? And so with that, you know, it's been a pleasure to work with, you know, the Dillman folks the building family and also Bobby and the rest of the team we've done a lot of really cool programs such as strategic. Eating programs you guys have built a lot of really bespoke capabilities for us on your guys's platform so that we can actually deliver happiness to our customers\", \"success is inherently tied together, right? And so with that, you know, it's been a pleasure to work with, you know, the Dillman folks the building family and also Bobby and the rest of the team we've done a lot of really cool programs such as strategic. Eating programs you guys have built a lot of really bespoke capabilities for us on your guys's platform so that we can actually deliver happiness\", 'success', 'drive a video first experience,', 'success', 'zoom', 'collaboration and communication should be seamless,', \"add participants. So it's added to their calendar and there's a I'm just going to Breeze through it and then a visitor it preserves the room and then you're golden.\", '20% and I would hear my interpreter voice', 'native control integration. How much can the going to be customized', 'they were built from the ground up all the way down', 'corporate', 'IP', 'collaboration and communication should be seamless, even when the needs for collaboration communication and change in real time.', 'corporate background', 'reduction within', 'they were built from the ground up all the way down to the OS so their Tech', 'My pleasure', 'about', 'IP Control', 'reduction within the carbon', 'My pleasure.', 'conference room', 'enterprise-grade video conferencing without Enterprise size', 'native control integration. How much can the going to be customized? Okay, we talked about that you are going to be using IP Control', 'video first', 'scheduling display,', 'native control integration. How much can the going to be customized? Okay, we talked about that you are going to be using IP', 'enterprise-grade video conferencing', 'add participants.', \"enterprise-grade video conferencing without Enterprise size complications. These are for people who don't want to do. Thinking they don't want to add multiple cameras. They're just like give me a box and I just wanted to plug it in real quick. And I want to do a hundred of these at skill\", 'native', 'success is inherently tied together', 'communication and change', 'g a so if you guys have scheduling display,', 'zoom platform is a whole', 'ability to scale to different use cases.', \"add participants. So it's added to their calendar and there's a I'm just going to Breeze through it and then a visitor\", 'zoom', 'My', 'g a', \"about 90% of use cases from what I've seen.\", 'zoom rooms', 'like an', 'success is really going to be tied to our customer', 'okay', 'she is a bit of a rock star when it comes to being able to get things', 'reloading Keys', \"IP Control. So that will be being bailed out more and more. Correct. \\n\\n- (27:28)  SPEAKER: Esther Yoon\\nEverything is IP based as long as they can I pee command. Yep. All right kids Optical device. Yep. \\n\\n- (27:36)  SPEAKER: Melissa Dillman\\nSo that'll be great. One of the other things that came out was the simultaneous interpretation\", 'scheduling display, I would recommend testing it out you upload the map as a jpeg. Or PNG directly through the zoom', 'g a so if you guys have scheduling', 'she is a bit of a rock star when it comes', 'conference room Solutions joining us today coming off the heels of zootopia as well, which is kind of a big deal in the zoom', 'like an extra cost. Now. We have zoom rooms native Native room control integration.', 'scheduling display, I would recommend testing it out you upload the map', 'video first experience,', 'reduction within the', '$50 gift', '20% and I would hear my interpreter', \"90% of use cases from what I've seen.\", 'conference room Solutions joining us today coming off the heels of zootopia as well, which is kind of a big deal in the zoom World shuts down, San Jose. Pretty good.', 'conference room Solutions joining us today coming off the heels of zootopia as well, which is kind of a big deal in the zoom World shuts down, San Jose. Pretty good. What we really wanted to do was just have a little pet about coming up the hill so that show on what is in the future for the next coming up on Zoom.', 'reloading Keys.', \"zoom platform is a whole and I wanted to thank we've got Esther Yoon who is the product marketing manager for conference room Solutions joining us today coming off the heels of zootopia as well, which is kind of a big deal in the zoom\", 'success is inherently', 'an', 'scheduling display, I would recommend testing it out you upload the map as a jpeg. Or PNG directly through the zoom admin portal.', 'an extra cost. Now. We have zoom rooms native Native room control integration.', 'extra cost. Now. We have zoom rooms native Native room control integration.', 'carbon footprint because we can do everything via video.', 'carbon footprint hmm in talking about that reduction within the carbon footprint because we can do everything via video.', 'carbon footprint hmm in talking about that reduction within the carbon footprint', 'footprint because we can do everything via video.', 'carbon footprint hmm in talking about that reduction']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg9xDvXXWPS3"
      },
      "source": [
        "s = [PrelimPrediction(feature_index=0, start_index=32, end_index=32, start_log_prob=-9.368597984313965, end_log_prob=-0.3939220607280731), PrelimPrediction(feature_index=0, start_index=240, end_index=241, start_log_prob=-9.159207344055176, end_log_prob=-1.1434053182601929), PrelimPrediction(feature_index=1, start_index=404, end_index=404, start_log_prob=-9.20814323425293, end_log_prob=-1.3890942335128784), PrelimPrediction(feature_index=1, start_index=368, end_index=369, start_log_prob=-9.735785484313965, end_log_prob=-0.9803922176361084), PrelimPrediction(feature_index=0, start_index=240, end_index=249, start_log_prob=-9.159207344055176, end_log_prob=-2.2522521018981934), PrelimPrediction(feature_index=0, start_index=240, end_index=253, start_log_prob=-9.159207344055176, end_log_prob=-2.268369197845459), PrelimPrediction(feature_index=1, start_index=368, end_index=368, start_log_prob=-9.735785484313965, end_log_prob=-1.779595136642456), PrelimPrediction(feature_index=1, start_index=368, end_index=377, start_log_prob=-9.735785484313965, end_log_prob=-1.868096113204956), PrelimPrediction(feature_index=0, start_index=240, end_index=240, start_log_prob=-9.159207344055176, end_log_prob=-2.5244574546813965), PrelimPrediction(feature_index=0, start_index=240, end_index=255, start_log_prob=-9.159207344055176, end_log_prob=-2.6515278816223145), PrelimPrediction(feature_index=1, start_index=184, end_index=185, start_log_prob=-9.782259941101074, end_log_prob=-2.59633207321167), PrelimPrediction(feature_index=0, start_index=32, end_index=44, start_log_prob=-9.368597984313965, end_log_prob=-3.2390778064727783), PrelimPrediction(feature_index=1, start_index=368, end_index=386, start_log_prob=-9.735785484313965, end_log_prob=-2.9417340755462646), PrelimPrediction(feature_index=0, start_index=32, end_index=36, start_log_prob=-9.368597984313965, end_log_prob=-3.5748379230499268), PrelimPrediction(feature_index=0, start_index=32, end_index=62, start_log_prob=-9.368597984313965, end_log_prob=-3.6320126056671143)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufCs-_5qW89u"
      },
      "source": [
        "PrelimPrediction(feature_index=0, start_index=32, end_index=32, start_log_prob=-9.368597984313965, end_log_prob=-0.3939220607280731)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVFcascSW2kx"
      },
      "source": [
        "ss = [['awesome', -9.762520045042038], ['consumer experience', -10.302612662315369], ['Melissa', -10.597237467765808], ['IP based', -10.716177701950073], ['consumer experience just like you would set up a display', -11.41145944595337], ['consumer experience just like you would set up a display with a sound system', -11.427576541900635], ['IP', -11.515380620956421], ['IP based as long as they can I pee', -11.603881597518921], ['consumer', -11.683664798736572], ['consumer experience just like you would set up a display with a sound system at home', -11.81073522567749], ['firmware', -12.378592014312744], ['awesome you covered a lot but you know what you left out.', -12.607675790786743], ['IP based as long as they can I pee command. Yep. All right kids Optical', -12.67751955986023], ['awesome you covered a lot', -12.943435907363892], ['awesome you covered a lot but you know what you left out. - (20:13)  SPEAKER: Esther Yoon\\nWhat the phone', -13.000610589981079]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYfz4NiwWffU",
        "outputId": "b2abec94-6b15-4cb8-9734-044d5dd59b29"
      },
      "source": [
        "for item in ss:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['awesome', -9.762520045042038]\n",
            "['consumer experience', -10.302612662315369]\n",
            "['Melissa', -10.597237467765808]\n",
            "['IP based', -10.716177701950073]\n",
            "['consumer experience just like you would set up a display', -11.41145944595337]\n",
            "['consumer experience just like you would set up a display with a sound system', -11.427576541900635]\n",
            "['IP', -11.515380620956421]\n",
            "['IP based as long as they can I pee', -11.603881597518921]\n",
            "['consumer', -11.683664798736572]\n",
            "['consumer experience just like you would set up a display with a sound system at home', -11.81073522567749]\n",
            "['firmware', -12.378592014312744]\n",
            "['awesome you covered a lot but you know what you left out.', -12.607675790786743]\n",
            "['IP based as long as they can I pee command. Yep. All right kids Optical', -12.67751955986023]\n",
            "['awesome you covered a lot', -12.943435907363892]\n",
            "['awesome you covered a lot but you know what you left out. - (20:13)  SPEAKER: Esther Yoon\\nWhat the phone', -13.000610589981079]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cmo7gSSZ879"
      },
      "source": [
        "sss = [['Lashawn Jackson.', -0.0833195741288364], ['Lashawn', -5.618622794747353], ['Lashawn Jackson', -5.979353919625282], ['Jackson.', -6.4887288408353925], ['shawn Jackson.', -7.439827188849449], ['It is Lashawn Jackson.', -7.570564657449722], ['La', -10.366392627358437], ['Lashawn Jackson. Thank you very much.', -10.664638057351112], ['It is Lashawn', -11.249517917633057], ['It is Lashawn Jackson', -11.37743854522705], ['Jackson', -11.518480777740479], ['It is Lashawn Jackson. Thank you very much.', -11.99608564376831], ['It is La', -12.521013259887695], ['shawn', -12.77404499053955], ['shawn Jackson', -13.511877536773682], ['shaw', -18.506267070770264]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q-cAMFdaLzE",
        "outputId": "4ea21457-2103-4e36-f8af-a216bd1cfe63"
      },
      "source": [
        "for item in sss:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Lashawn Jackson.', -0.0833195741288364]\n",
            "['Lashawn', -5.618622794747353]\n",
            "['Lashawn Jackson', -5.979353919625282]\n",
            "['Jackson.', -6.4887288408353925]\n",
            "['shawn Jackson.', -7.439827188849449]\n",
            "['It is Lashawn Jackson.', -7.570564657449722]\n",
            "['La', -10.366392627358437]\n",
            "['Lashawn Jackson. Thank you very much.', -10.664638057351112]\n",
            "['It is Lashawn', -11.249517917633057]\n",
            "['It is Lashawn Jackson', -11.37743854522705]\n",
            "['Jackson', -11.518480777740479]\n",
            "['It is Lashawn Jackson. Thank you very much.', -11.99608564376831]\n",
            "['It is La', -12.521013259887695]\n",
            "['shawn', -12.77404499053955]\n",
            "['shawn Jackson', -13.511877536773682]\n",
            "['shaw', -18.506267070770264]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNF226KaaH-u"
      },
      "source": [
        "# Answer 2\n",
        "s = ['awesome', 'consumer experience', 'Melissa', 'IP based', 'consumer experience just like you would set up a display', 'consumer experience just like you would set up a display with a sound system', 'IP', 'IP based as long as they can I pee', 'consumer', 'consumer experience just like you would set up a display with a sound system at home', 'firmware', 'awesome you covered a lot but you know what you left out.', 'IP based as long as they can I pee command. Yep. All right kids Optical', 'awesome you covered a lot', 'awesome you covered a lot but you know what you left out. - (20:13)  SPEAKER: Esther Yoon\\nWhat the phone']\n",
        "\n",
        "[[90, 97], [784, 803], [2019, 2026], [1914, 1922], [784, 840], [784, 860], [1914, 1916], [1914, 1948], [784, 792], [784, 868], [1156, 1164], [90, 147], [1914, 1985], [90, 115], [90, 194]]\n",
        "\n",
        "[[[90, 2026], \"awesome you covered a lot but you know what you left out. - (20:13)  SPEAKER: Esther Yoon\\nWhat the phone ah, okay. - (20:17)  SPEAKER: Melissa Dillman\\nJust watched your video. - (20:19)  SPEAKER: Esther Yoon\\nAgain, and how cool you were with the phone? - (20:22)  SPEAKER: Melissa Dillman\\nThat's hilarious and I have to say we just so zootopia last year. We saw the phone, right? We saw some come this year. We implemented it. And yeah, obviously, I'm pretty I'm pretty deep with zoom. I use it everyday. I was a little concerned. Like, how do I have a call and how do I get it to a meeting and I have to say it's amazingly intuitive just like everything else was in right? It's kind of like a consumer experience just like you would set up a display with a sound system at home. And they had a video like Polly had a video where they were showing the insulation. It literally took two minutes. And then enhanced management, I already covered this but you know through the zoom admin portal click update and it does the entire software stack even down to the device firmware. So in a nutshell the way we're starting to structure. Our Zoom rooms is is really, you know, first and foremost it comes down to what does the customer want if they want that flexibility in their Huddle's basis and conference rooms. We're going to recommend them a Windows iot computer Mac or something like that so they can have that bespoke experience if they want that. So I watched a bunch of them when I came home as you know to get a chance to watch it again or enjoy it again. So it's awesome some rooms native control integration. How much can the going to be customized? Okay, we talked about that you are going to be using IP Control. So that will be being bailed out more and more. Correct. - (27:28)  SPEAKER: Esther Yoon\\nEverything is IP based as long as they can I pee command. Yep. All right kids Optical device. Yep. - (27:36)  SPEAKER: Melissa\"]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umM2GnODaOXv",
        "outputId": "27435317-a5cf-420d-f3c7-1ff52547173e"
      },
      "source": [
        "for item in s:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "awesome\n",
            "consumer experience\n",
            "Melissa\n",
            "IP based\n",
            "consumer experience just like you would set up a display\n",
            "consumer experience just like you would set up a display with a sound system\n",
            "IP\n",
            "IP based as long as they can I pee\n",
            "consumer\n",
            "consumer experience just like you would set up a display with a sound system at home\n",
            "firmware\n",
            "awesome you covered a lot but you know what you left out.\n",
            "IP based as long as they can I pee command. Yep. All right kids Optical\n",
            "awesome you covered a lot\n",
            "awesome you covered a lot but you know what you left out. - (20:13)  SPEAKER: Esther Yoon\n",
            "What the phone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S2pY8pibsNt",
        "outputId": "7985a3e0-d061-4068-f245-7a75c01bb595"
      },
      "source": [
        "for item in hmm:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Zoom?', -9.047911822795868], [\"Zoom? So I've heard that's there for a long time\", -11.476564645767212], ['meat', -11.548356056213379], ['meat and they announced the neat bar and the neat board which is a touch display and then Paul Lee', -12.239153861999512], ['meat and they announced the neat bar and the neat board', -12.789748191833496], ['Zoom', -12.792715072631836], [\"meat and they announced the neat bar and the neat board which is a touch display and then Paul Lee announced. Their Appliance x-series. So there is an x 30 which is for smaller rooms. And then there's an x 54 larger rooms\", -13.159703254699707], [\"meat and they announced the neat bar and the neat board which is a touch display and then Paul Lee announced. Their Appliance x-series. So there is an x 30 which is for smaller rooms. And then there's an x 54\", -13.182027816772461]]\n",
            "[['Not much text', -6.257963299751282], ['Not much text absolutely', -7.6872111558914185], ['Not much text absolutely.', -8.112068176269531], ['Not much', -9.247757911682129], ['text', -9.61880373954773], ['native control', -10.15002429485321], ['text absolutely', -11.084724187850952], ['native', -11.391365051269531], ['native control integration.', -11.546382904052734], ['native control integration. How much can the going to be customized? Okay, we talked about that you are going to be using IP Control. So that will be being bailed out more and more. Correct. - (27:28', -12.503212928771973], ['Not', -12.71066427230835], ['text absolutely.', -12.826075315475464]]\n",
            "[['awesome', -9.76253005862236], ['consumer experience', -10.302623867988586], ['Melissa', -10.597243785858154], ['IP based', -10.716168880462646], ['consumer experience just like you would set up a display', -11.411467552185059], ['consumer experience just like you would set up a display with a sound system', -11.42758560180664], ['IP', -11.515376567840576], ['IP based as long as they can I pee', -11.60387659072876], ['consumer', -11.683674812316895], ['consumer experience just like you would set up a display with a sound system at home', -11.810742378234863], ['firmware', -12.378593444824219], ['awesome you covered a lot but you know what you left out.', -12.607691526412964], ['IP based as long as they can I pee command. Yep. All right kids Optical', -12.677516460418701], ['awesome you covered a lot', -12.943442106246948], ['awesome you covered a lot but you know what you left out. - (20:13)  SPEAKER: Esther Yoon\\nWhat the phone', -13.000631093978882]]\n",
            "[['carbon footprint', -9.92733883857727], ['carbon footprint', -10.246058642864227], ['LED Wireless belt', -10.469384551048279], ['two', -10.695582866668701], ['LED Wireless', -10.805814146995544], ['carbon footprint hmm in talking about that reduction within the carbon footprint', -10.818270921707153], ['LED Wireless belt on owes a debt that we controlled from the zoo', -11.043057799339294], ['what all is new with zoom', -11.071001291275024], ['LED', -11.258073329925537], ['two Rockstar kind of moments, but let me get to what what all of our guests want to know. So I see a couple of questions like well, yeah, what all is new with zoom', -11.387782573699951], ['carbon', -11.891735315322876], [\"carbon footprint because we can do everything via video. Yep. You don't have to travel the miles.\", -12.331176042556763], ['what all is new', -12.419607400894165], [\"two Rockstar kind of moments, but let me get to what what all of our guests want to know. So I see a couple of questions like well, yeah, what all is new with zoom and what we can offer to our customers. I think we've covered a lot\", -12.742032527923584], [\"what all is new with zoom and what we can offer to our customers. I think we've covered a lot of that\", -12.98731541633606], ['what all is new with zoom and', -13.143167734146118], ['carbon footprint hmm in talking about that reduction', -13.203859567642212], ['carbon footprint hmm in talking about that reduction within the carbon footprint because we can do everything via video. Yep.', -14.054760932922363], ['carbon', -14.113659858703613], [\"carbon footprint because we can do everything via video. Yep. You don't have to travel the miles. I hate to fly. So anytime I can do it via video.\", -14.142250061035156]]\n",
            "[['90%', -4.601068966090679], ['about 90%', -5.519722282886505], ['wherever they are.', -5.555246517062187], ['right', -6.315600872039795], ['about 90% of use cases', -6.4200193881988525], ['drag this wherever they are.', -6.572456955909729], ['drag', -6.920649170875549], [\"Yep, and that's really great because if you think about it between these two partners you have your Sighs or sorry small conference rooms with the x30 midsize conference rooms with the x 50 and if you want to zoom rooms for touch you have the d-10 so it hits about 90% of use cases\", -7.180545330047607], [\"Yep, and that's really great\", -7.1822190284729], ['drag this', -7.536836266517639], ['about 90% of', -7.581336259841919], ['90% of use cases', -7.825608730316162], [\"Yep, and that's really great because if you think about it between these two partners you have your Sighs or sorry small conference rooms with the x30 midsize conference rooms with the x 50 and if you want to zoom rooms for touch you have the d-10 so it hits about 90% of use cases from what I've seen.\", -8.127978801727295], ['90% of', -8.6107497215271], ['wherever they are. And so essentially once that room is assigned to a zoom room.', -8.774301767349243], [\"about 90% of use cases from what I've seen.\", -8.849979639053345], ['Yep,', -8.969591617584229], [\"Yep, and that's really great because if you think about it between these two partners you have your Sighs or sorry small conference rooms with the x30 midsize conference rooms with the x 50 and if you want to zoom rooms for touch you have the d-10 so it hits about 90% of use cases from\", -8.977296352386475], [\"90% of use cases from what I've seen.\", -9.266808032989502]]\n",
            "[['Can we customize?', -5.7473345175385475], ['simultaneous interpretation. Yep. So, okay. It left us all scratching our heads. How does that work? How much of the GUI? Can we customize?', -7.418205499649048], ['simultaneous interpretation.', -7.689653635025024], ['different modalities of a different of a control system', -8.475893437862396], ['different modalities', -8.798646450042725], ['simultaneous', -8.827630281448364], ['Can we customize', -9.269689559936523], ['simultaneous interpretation', -9.428966760635376], ['different modalities of a different of a control system that are preset.', -10.982629299163818], ['different modalities of a different of a control system that are preset', -11.192640781402588], [\"Can we customize? It's a great question.\", -11.239940166473389], ['different modalities of a different of a control', -11.711997509002686]]\n",
            "[['meeting', -3.6630749379401095], ['meeting', -4.0998002253472805], ['into the meeting', -4.120570285245776], ['get it to a meeting', -4.398133777081966], ['get it to a meeting and', -9.296708583831787], ['get it', -9.394779205322266], ['into', -10.676473140716553], ['into the', -11.78748893737793], ['into the meeting and', -11.833650588989258], [\"into the meeting and I'm like, we're already there.\", -12.383305549621582], [\"meeting and I'm like, we're already there.\", -13.887256860733032], ['meeting and', -14.0686514377594]]\n",
            "[['you can have remote', -5.734853982925415], ['now with our Zoom rooms appliances you can have remote', -6.050734639167786], ['now with our Zoom rooms appliances', -6.77375328540802], ['You can have a lockdown OS designed to only run', -6.850412368774414], ['You can have a lockdown OS designed to only run Zoom rooms.', -6.857361793518066], ['you can have remote for more updates', -6.881643533706665], ['You can have a lockdown OS', -6.985501289367676], ['you can have remote for more updates for all the peripherals on the appliances.', -6.990528345108032], ['But now with our Zoom rooms appliances you can have remote', -7.262768507003784], ['But now with our Zoom rooms appliances you can have remote for more updates for all the peripherals on the appliances.', -7.4843785762786865], ['now', -7.614337921142578], ['But now with our Zoom rooms appliances', -7.664271116256714], ['You can have a lockdown OS designed to only run Zoom rooms. And then you can have that Global streamlined Hardware procurement option,', -7.670332908630371], ['now with our Zoom rooms appliances you can have remote for more updates for all the peripherals on the appliances.', -7.760952949523926], ['now with our Zoom rooms appliances you can have remote for more updates', -7.968855857849121], ['But now with our Zoom rooms appliances you can have remote for more updates', -8.420095205307007], ['You can have a lockdown', -8.606346130371094], ['But now with our Zoom rooms appliances you can have remote for more updates for all the peripherals', -8.7790048122406]]\n",
            "[[\"you're able to have a virtual background\", -7.321069121360779], [\"listen to you and translate what you're saying into a interpreter Channel\", -7.32358056306839], [\"you're able to have a virtual background for your Zoom room.\", -8.031900763511658], ['listen to you and translate', -8.097331762313843], [\"you're able to have a virtual background for your\", -8.344749808311462], [\"Hey, I want to hear the Italian translation of what Bobby saying I could find it if there's an interpreter signed\", -8.871323585510254], [\"I could find it if there's an interpreter signed and what happens is Bobby's voice would go down to 20% and I would hear my interpreter voice at 80%\", -9.016065001487732], [\"I could find it if there's an interpreter signed\", -9.279993414878845], ['Hey, I want to hear the Italian translation of what Bobby', -9.297881126403809], [\"I could find it if there's an interpreter\", -9.33334493637085], ['Hey, I want to hear the Italian translation of what Bobby saying', -9.355152130126953], ['Hey, I want to hear the Italian translation', -9.454768180847168], [\"Hey, I want to hear the Italian translation of what Bobby saying I could find it if there's an interpreter\", -9.59704303741455], ['listen', -9.624783277511597], ['listen to you', -9.729173421859741]]\n",
            "[['Esther Yoon', -1.15748560288921], ['marketing', -4.478865519165993], ['conference room Solutions', -5.04610363394022], ['product marketing', -5.678847908973694], ['product marketing manager', -5.94415819644928], ['marketing manager', -6.4505109786987305], ['Esther', -6.504528045654297], ['Sam Cooke Keiko', -7.432068288326263], ['Sam Cooke', -7.672475278377533], ['product marketing manager for conference room Solutions', -8.337467670440674], ['Esther Yoon who is the product marketing', -8.355233192443848], ['training', -8.664039179682732], ['Esther Yoon who is the product marketing manager', -9.045952796936035], ['marketing manager for conference room Solutions', -9.267640113830566], ['product', -9.336512088775635], ['conference room', -9.375620365142822], ['conference room Solutions joining us today', -9.48277473449707], ['Esther Yoon who is the product marketing manager for conference room Solutions', -9.628448009490967], ['conference room Solutions joining us', -10.129884243011475], ['working', -12.459805488586426], [\"Sam Cooke Keiko who's on our PSO\", -13.971820831298828], [\"Sam Cooke Keiko who's on our PSO team\", -14.23678731918335], ['Sam', -14.702472686767578]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NfjRg49WYgt"
      },
      "source": [
        "# get the qid and get the respective answer\n",
        "reader_ans = reader_out['customer_1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXuY1N-ThfZ-"
      },
      "source": [
        "# Step 3: THE GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP5cwZWjWYeb"
      },
      "source": [
        "# get actual content for the top candidates\n",
        "paragraphs = list(reader_out.values())\n",
        "meta_data = [{\"document_id\": 'customer_1', \"paragraph_id\": None}]\n",
        "\n",
        "documents = []\n",
        "for para, meta in zip(paragraphs, meta_data):\n",
        "    documents.append(\n",
        "        Document(\n",
        "            id=meta[\"document_id\"],\n",
        "            text=para,\n",
        "            meta=meta.get(\"meta\", {})\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYVUE6IHzI7_",
        "outputId": "d0933dc5-ab10-4dc3-e6d7-1e5d79c277a2"
      },
      "source": [
        "documents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': 'Necker Island', 'score': None, 'probability': None, 'question': None, 'meta': {}, 'embedding': None, 'id': 'customer_1'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Smw1I6tWYcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845,
          "referenced_widgets": [
            "b0d23f02eb3149c59e0ee043e022d886",
            "fcf5b5f6134243ff847be7e247ba9c4c",
            "887566e41e424df8a9546b885a5f39de",
            "c5302aa8eed143b9a70fe86e31efb60e",
            "62073b25c301400695bc0fc9d914c023",
            "5bec0694701b486bb66410e3560a053c",
            "40bd66ba0118461f866560dbb8b1d8a3",
            "181ae90182cc49ec875154ee89cb1ad1",
            "d64d4082b48e4b9083007e631bd335d0",
            "3cf10ac7ad6e48778f86bb76f5e50900",
            "469a4fa497e645c7a393d3b8bc567569",
            "c5495136cc974a2691260482143fd0c1",
            "5d42c9f18c2f4419bc5310724e6f5d62",
            "828f70cc5e7b4efa97582e6a840cae9b",
            "fa975f9b6f684d948621522b6250e539",
            "0b8e5d9d07ab4436a853c93df90e68e7",
            "6e3ee7f4b4d947f9b9fb298444b3f4e5",
            "dbcb4876d960430cafa22988088c2129",
            "b777c94bf0a94a85b87136a42894dbef",
            "37d4aa4253014d638dea7a34ca9c67a7",
            "3b46b15c0e1c4bdd87453971411f4e44",
            "ab1d6c4785f34c15bb7f9b9c4f3aadc5",
            "eeaf7d2752fc465a89a97b0ba373716b",
            "5a2f5895a2da40f1b946a02c039b466b",
            "afa61f857f704ceb8321564073b862aa",
            "5c8759bea55e41f89b1a9603e5ab26ee",
            "60188a15d3a241158e05ffab3a55d62b",
            "c65a4bcd5e544456971c88021e8a1b95",
            "d77a7787d0264766908955bdeccd4610",
            "2e902f0ef8094d9ab6dc26c744851f08",
            "2c711d48a3174f4d97998acd164776e7",
            "69569dc3e6f9489cae5f387a88653472",
            "8cb96ae46a0b41df87acfd82bb14266b",
            "6d0be926ebc44d69b68c2290ede82308",
            "f8335ad339d944f9b76f818833b1880e",
            "82f645197ac94775a1158e2be80271cf",
            "df67b9c7a35949da84944e30004f13d2",
            "f44e5a7e55624cc4a94bc83e95aba635",
            "915fec6b015e40d387a20a0533078beb",
            "667693de601f48e89f874c86de243ff5",
            "e19d482e8e994263be48d3539e86c84e",
            "dd8fb1d643e6420f85cdaa15621e9700",
            "4914acf461664a6cac44edbd57a8ded4",
            "cf7ded00be7d4ea19fee1f6e242ad4ab",
            "c3aa00b5ba854190933a238c3af992d0",
            "4711168e022d4246a64ea11ce14071df",
            "783311328787493cad2730db5eb8ca23",
            "5ad9958a02ec4b03afb8ebccd39ff205",
            "595c0888f3ad4dd49898a09fd8d28d40",
            "9f13c95751cd4190830bbf62aa67430b",
            "81feb01b4d14443d8655c7a741f1d8bc",
            "a20c0a8e621246e0ba610496d53f3c95",
            "336b1ef46a12410ab5fd8370e409a4ad",
            "772e7d0e0c9f4e38a0f5ca2c6bff97d9",
            "b04bc1d81e8843d4ba625385e1860229",
            "49dfde9d51ca4e54a2972b1ed59b2c32",
            "55859d3a36d4425b806152f097fb9c43",
            "88245d6d451d40979c37d5ff333238c3",
            "1d95d8db5b654355a978211c042eea20",
            "547085a7591c40d2be45916bf377d025",
            "91ef161cddc94f8f97c9606e2815dfb6",
            "3708151d1a5a40608f7e32d292b5bbb4",
            "4dd0e61e52bc4417aa0f87e6b9d25f60",
            "09681a23511d4b20864ee98e9f3da0fb",
            "eb490194def14fa4973d373f13033177",
            "0b83a23ff1344e7dac4ff639c5bb858e",
            "0ca8e3a40ceb47ee806c182fac2c9995",
            "d90ad400a78c4a7aa9907b86381d6568",
            "c5cedacc97064442ac5e7618414f69b4",
            "9a1f663618d04f09a6c9b6cd6106c6bd",
            "3fac9e5a585f412f939c28db77b27bfc",
            "d334897e3aa04bba99612ae2e57c6ed8"
          ]
        },
        "outputId": "6a75cc80-6fe1-4bc5-bf58-bf3f6d7e5452"
      },
      "source": [
        "# Initialize RAG Generator\n",
        "generator = RAGenerator(\n",
        "    model_name_or_path=\"facebook/rag-token-nq\",\n",
        "    use_gpu=True,\n",
        "    top_k=1,\n",
        "    max_length=200,\n",
        "    min_length=2,\n",
        "    embed_title=False,\n",
        "    num_beams=5,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:49:56 - INFO - filelock -   Lock 140010000895504 acquired on /root/.cache/huggingface/transformers/6337b0203e20d15c98f5e500e1e673c74e71bb8617b2753a53663b9b8e6dfc1a.59948e1fef260da10a0cecb8b6862373c32f40001848a63f985ab4f9d787f3f1.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0d23f02eb3149c59e0ee043e022d886",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=4602.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:49:57 - INFO - filelock -   Lock 140010000895504 released on /root/.cache/huggingface/transformers/6337b0203e20d15c98f5e500e1e673c74e71bb8617b2753a53663b9b8e6dfc1a.59948e1fef260da10a0cecb8b6862373c32f40001848a63f985ab4f9d787f3f1.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bart/configuration_bart.py:178: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions.The config can simply be saved and uploaded again to be fixed.\n",
            "  f\"Please make sure the config includes `forced_bos_token_id={self.bos_token_id}` in future versions.\"\n",
            "06/17/2021 11:49:57 - INFO - filelock -   Lock 140009993585680 acquired on /root/.cache/huggingface/transformers/26cf899a0974235af1f84469ddd94d2ee83c803c23ecead93b511ce8a0744f5c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d64d4082b48e4b9083007e631bd335d0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:49:58 - INFO - filelock -   Lock 140009993585680 released on /root/.cache/huggingface/transformers/26cf899a0974235af1f84469ddd94d2ee83c803c23ecead93b511ce8a0744f5c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:49:58 - INFO - filelock -   Lock 140009993732176 acquired on /root/.cache/huggingface/transformers/d4df3c917efc1bf4cde9515ac4432cddf040d3ddacfad55c85445bc985f58ceb.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e3ee7f4b4d947f9b9fb298444b3f4e5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:49:59 - INFO - filelock -   Lock 140009993732176 released on /root/.cache/huggingface/transformers/d4df3c917efc1bf4cde9515ac4432cddf040d3ddacfad55c85445bc985f58ceb.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:49:59 - INFO - filelock -   Lock 140009993732176 acquired on /root/.cache/huggingface/transformers/445caa3aaff6c34b07acdff304db0c8468640baf1139f92c14270fc50cff2eb8.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afa61f857f704ceb8321564073b862aa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=48.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:49:59 - INFO - filelock -   Lock 140009993732176 released on /root/.cache/huggingface/transformers/445caa3aaff6c34b07acdff304db0c8468640baf1139f92c14270fc50cff2eb8.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:50:00 - INFO - filelock -   Lock 140009993499152 acquired on /root/.cache/huggingface/transformers/786598a0d343d4afb34b4f1ee17c14b58fb129abb0b3db386587bea52ffcfb11.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cb96ae46a0b41df87acfd82bb14266b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898822.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:50:00 - INFO - filelock -   Lock 140009993499152 released on /root/.cache/huggingface/transformers/786598a0d343d4afb34b4f1ee17c14b58fb129abb0b3db386587bea52ffcfb11.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:50:01 - INFO - filelock -   Lock 140009993742160 acquired on /root/.cache/huggingface/transformers/f5fdd0c4b41c985f791c9a230cdf051da1cf9480d84e0f5f617667760a24f50f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e19d482e8e994263be48d3539e86c84e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:50:01 - INFO - filelock -   Lock 140009993742160 released on /root/.cache/huggingface/transformers/f5fdd0c4b41c985f791c9a230cdf051da1cf9480d84e0f5f617667760a24f50f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:50:02 - INFO - filelock -   Lock 140010000019600 acquired on /root/.cache/huggingface/transformers/73d054acda2f2b44f075a0d391ed392e6c53f78356bbf279581786ae35a79e4d.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "595c0888f3ad4dd49898a09fd8d28d40",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=772.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:50:02 - INFO - filelock -   Lock 140010000019600 released on /root/.cache/huggingface/transformers/73d054acda2f2b44f075a0d391ed392e6c53f78356bbf279581786ae35a79e4d.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:50:02 - INFO - filelock -   Lock 140009993743824 acquired on /root/.cache/huggingface/transformers/a2f0711fd4c199beaf3ff2626c4a723e107e79a8d065099a3d24e7aa361c0f18.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55859d3a36d4425b806152f097fb9c43",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=26.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:50:03 - INFO - filelock -   Lock 140009993743824 released on /root/.cache/huggingface/transformers/a2f0711fd4c199beaf3ff2626c4a723e107e79a8d065099a3d24e7aa361c0f18.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:50:04 - INFO - filelock -   Lock 140009845683280 acquired on /root/.cache/huggingface/transformers/6288b70ae87b5989f6801120b25a2f6ccf2e88c2f622d94805717b5b043ff71b.bceb1ea6e95a1b39d0a717f315800a39b0847e5cdeeb42615fb498775f632ee6.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb490194def14fa4973d373f13033177",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2063600064.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/17/2021 11:50:45 - INFO - filelock -   Lock 140009845683280 released on /root/.cache/huggingface/transformers/6288b70ae87b5989f6801120b25a2f6ccf2e88c2f622d94805717b5b043ff71b.bceb1ea6e95a1b39d0a717f315800a39b0847e5cdeeb42615fb498775f632ee6.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RagTokenForGeneration were not initialized from the model checkpoint at facebook/rag-token-nq and are newly initialized: ['rag.generator.lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbKGtoY3WYXa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "1a83a9a6-af99-41de-ecdd-7a766b77522a"
      },
      "source": [
        "# Now generate answer from question and retrieved documents\n",
        "predicted_result = generator.predict(\n",
        "    query=\"From where Richard Branson was supposed to call?\",\n",
        "    documents=documents,\n",
        "    top_k=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-9b4303fe9875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"From where Richard Branson was supposed to call?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haystack/generator/transformers.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, query, documents, top_k)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;31m# Raw document embedding and set device of query_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mpassage_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_passage_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_docs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;31m# Query tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haystack/generator/transformers.py\u001b[0m in \u001b[0;36m_prepare_passage_embeddings\u001b[0;34m(self, docs, embeddings)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_embedding_required\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_prepare_passage_embeddings need a DPR instance as self.retriever to embed document\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_passages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: _prepare_passage_embeddings need a DPR instance as self.retriever to embed document"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt-6nMcGWYVV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tReQ2d1mWYTD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}